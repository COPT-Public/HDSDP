\section{Dual and Homogeneous Dual Scaling Algorithm }\label{sec3}

In this section, we briefly review the dual-scaling algorithm in
{{\texttt{DSDP}}}, and give its interpretation through Newton's method on the KKT system. Then we show how it can be naturally generalized to the embedding formation.
\subsection{Dual-scaling Algorithm}
Dual-scaling method
{\cite{benson1999mixed}} works under three conditions. {\textbf{1)}} the
 data $\left\{ \mathbf{A}_i \right\}$ are linearly independent.
{\textbf{2)}} Both $(P)$ and $(D)$ admit an interior point
solution. {\textbf{3)}} an interior dual feasible solution $\left( \mathbf{y}^0,
\mathbf{S}^0 \right) \in \mathcal{F}^0 (D)$ is known. The first two conditions imply
strong duality and the existence of an optimal primal-dual solution pair $\left(
\mathbf{X}^{\ast}, \mathbf{y}^{\ast}, \mathbf{S}^{\ast} \right)$ satisfying complementarity
$\left\langle \mathbf{X}^{\ast}, \mathbf{S}^{\ast} \right\rangle = 0$. Also, the central path $\mathcal{C} (\mu) :=
\left\{ \left( \mathbf{X}, \mathbf{y}, \mathbf{S} \right) \in \mathcal{F}^0 (P) \times \mathcal{F}^0
(D) : \mathbf{X} \mathbf{S} = \mu \mathbf{I} \right\}$ is well-defined, which serves as a foundation of
path-following approaches. Given a centrality parameter $\mu$, 
dual method starts from
$\left( \mathbf{y}, \mathbf{S} \right) \in \mathcal{F}^0(D)$ and takes Newton's step towards the perturbed KKT system $\mathcal{A} \mathbf{X} = \mathbf{b},
\mathcal{A}^{\ast} \mathbf{y} + \mathbf{S} = \mathbf{C}$ and $\mathbf{X} \mathbf{S} = \mu \mathbf{I}$
\begin{align}\label{dsdpnewton}
  \mathcal{A} \left( \mathbf{X} + \Delta \mathbf{X} \right) & = \mathbf{b} \nonumber \\
  \mathcal{A}^{\ast} \Delta \mathbf{y} + \Delta \mathbf{S} & = \textbf{0} \\
  \mu \mathbf{S}^{- 1} \Delta \mathbf{S} \mathbf{S}^{- 1} + \Delta \mathbf{X} & = \mu \mathbf{S}^{- 1} - \mathbf{X} \nonumber, 
\end{align}
\revised{where the last relation linearizes $\mathbf{X} = \mu \mathbf{S}^{- 1}$ instead of $\mathbf{X} \mathbf{S} =
\mu \mathbf{I}$ and $\mathbf{S}^{- 1}$ is known as a scaling matrix. 
By geometrically driving $\mu$ to 0, dual-scaling eliminates (primal) infeasibility, approaches optimality, and finally solves the problem to some $\varepsilon$-optimal solution $(\mathbf{y_\varepsilon}, \mathbf{S_\varepsilon}) \in \mathcal{F}^0(D)$ such that $ \langle \mathbf{b}, \mathbf{y_\varepsilon} \rangle \geq  \langle \mathbf{b}, \mathbf{y^*} \rangle - \varepsilon$. Theory of dual potential reduction shows  an $\varepsilon$-optimal solution can be obtained in $\mathcal{O}(\log(1/\varepsilon))$ iterations ignoring dimension dependent constants.}

An important feature of dual-scaling is that $\mathbf{X}$ and $\Delta \mathbf{X}$
vanish in the Schur complement of \eqref{dsdpnewton}

\begin{eqnarray} \label{dsdpM}
	\left(\begin{array}{ccc}
     \left\langle \mathbf{A}_1, \mathbf{S}^{- 1} \mathbf{A}_1 \mathbf{S}^{- 1} \right\rangle & \cdots &
     \left\langle \mathbf{A}_1, \mathbf{S}^{- 1} \mathbf{A}_m \mathbf{S}^{- 1} \right\rangle\\
     \vdots & \ddots & \vdots\\
     \left\langle \mathbf{A}_m, \mathbf{S}^{- 1} \mathbf{A}_1 \mathbf{S}^{- 1} \right\rangle & \cdots &
     \left\langle \mathbf{A}_m, \mathbf{S}^{- 1} \mathbf{A}_m \mathbf{S}^{- 1} \right\rangle
   \end{array}\right) \Delta \mathbf{y} = \frac{1}{\mu} \mathbf{b} - \mathcal{A} \mathbf{S}^{- 1},
\end{eqnarray}\\
and the dual algorithm thereby avoids explicit reference to the primal
variable $\mathbf{X}$. For brevity we sometimes denote the left-hand side matrix in \eqref{dsdpM} by $\mathbf{M}$.
If $\left\{ \mathbf{A}_i \right\}, \mathbf{C}$ are sparse, any feasible dual slack $\mathbf{S} = \mathbf{C}
-\mathcal{A}^{\ast} \mathbf{y}$ inherits the aggregated sparsity pattern from the data and it is
computationally cheaper to iterate in the dual space.
Another feature of dual-scaling is the availability of primal
solution by solving a projection subproblem at the end of the algorithm \cite{benson2008algorithm}. The
aforementioned properties characterize the behavior of the dual-scaling method.

However, the desirable theoretical properties of the dual method is not free.  First, an initial dual feasible solution is needed, while obtaining such a
solution is often as difficult as solving the original problem. Second, due to
a lack of information from the primal space, the dual-scaling has to be properly guided to avoid deviating from the central path. Last,
dual-scaling linearizes the highly nonlinear relation $\mathbf{X} = \mu \mathbf{S}^{- 1}$ and
this imposes strict constraint on the step length towards the Newton's direction. To overcome the aforementioned difficulties, {{\texttt{DSDP}}}
introduces artificial variables with big-$M$ penalty to ensure nonempty interior and a trivial dual feasible solution. Moreover, a potential function is introduced 
to guide the dual iterations. These solutions work well in practice and enhance {{\texttt{DSDP}}} greatly. For a complete description of the theoretical aspects of {{\texttt{DSDP}}} and
its delicate implementation, we refer the interested readers to
{\cite{benson2000solving, benson2008algorithm}}.

\revised{Although {{\texttt{DSDP}}} proves efficient in real practice, the big-$M$
method requires prior estimation of the optimal solution to avoid losing optimality. 
Also, a large penalty often leads to numerical difficulties and
misclassification of infeasible problems when the problem is ill-conditioned. 
Taking into account of In this paper, we propose to
leverage the well-known HSD embedding.}

\subsection{Dual-scaling Algorithm using Embedding Technique}
Now we are ready to introduce the idea behind {{\texttt{HDSDP}}}.
Given a standard form SDP, its homogeneous self-dual model is a skew symmetric system that contains the original problem data, whose non-trivial interior point solution certificates primal-dual feasibility. {\hdsdp} adopts the following simplified embedding system {\cite{potra1998homogeneous}}.
\begin{align}
  \mathcal{A} \mathbf{X} - \mathbf{b} \tau ={} & \textbf{0} \nonumber\\
  - \mathcal{A}^{\ast} \mathbf{y} + \mathbf{C} \tau - \mathbf{S} ={} & \textbf{0}  \\
  \mathbf{b}^{\top} \mathbf{y} - \langle \mathbf{C}, \mathbf{X} \rangle - \kappa ={}& 0 \nonumber\\
  \mathbf{X}, \mathbf{S} \succeq 0, \kappa, \tau  \geq{} & 0 \nonumber,
\end{align}
where $\kappa, \tau$ are homogenizing variables for infeasibility
detection. The central path of barrier parameter $\mu$ is given by
\begin{align}
  \mathcal{A} \mathbf{X} - \mathbf{b} \tau ={} & \textbf{0} \nonumber \\
  - \mathcal{A}^{\ast} \mathbf{y} + \mathbf{C} \tau - \mathbf{S} ={} & \textbf{0} \nonumber \\
  \mathbf{b}^{\top} \mathbf{y} - \langle \mathbf{C}, \mathbf{X} \rangle - \kappa ={} & 0 \nonumber \\
  \mathbf{X} \mathbf{S} = \mu \mathbf{I}, \kappa \tau ={} & \mu \label{hdsdpcompl} \\
  \mathbf{X}, \mathbf{S} \succeq 0, \kappa, \tau  \geq{} & 0. \nonumber
\end{align}
Here $\left( \mathbf{y}, \mathbf{S}, \tau \right)$ are jointly considered as dual variables.
Given an (infeasible) dual point $\left( \mathbf{y}, \mathbf{S}, \tau \right)$ such that $- \mathcal{A}^{\ast} \mathbf{y} +
\mathbf{C} \tau - \mathbf{S} = \mathbf{R}$, {{\texttt{HDSDP}}} selects a damping factor $\gamma \in
(0, 1]$ and takes Newton's step towards
\begin{align*}
  \mathcal{A} ( \mathbf{X} + \Delta \mathbf{X} ) - \mathbf{b} (\tau + \Delta \tau) & =\textbf{0}\\
  -\mathcal{A}^{\ast} ( \mathbf{y} + \Delta \mathbf{y}) + \mathbf{C} (\tau + \Delta \tau)
  - ( \mathbf{S} + \Delta \mathbf{S} ) & = - \gamma \mathbf{R}\\
  \mu \mathbf{S}^{- 1} \Delta \mathbf{S} \mathbf{S}^{- 1} + \Delta \mathbf{X} & = \mu \mathbf{S}^{- 1} - \mathbf{X},\\
  \mu \tau^{- 2} \Delta \tau + \Delta \kappa & =\mu \tau^{- 1} - \kappa,
\end{align*}
where, as in {{\texttt{DSDP}}}, we modify \eqref{hdsdpcompl} and linearize $\mathbf{X} = \mu \mathbf{S}^{- 1}$ and $\kappa
= \mu \tau^{- 1}$. We use this damping factor and the barrier parameter to manage a trade-off between dual infeasibility, centrality and optimality. 
If we set $\gamma = 0$, then the Newton's direction $\left( \Delta \mathbf{y}, \Delta \tau \right)$ is computed through
the following Schur complement.

\begin{eqnarray*}
  \Bigg(\begin{array}{cc}
    \mu \mathbf{M} & - \mathbf{b} - \mu \mathcal{A} \mathbf{S}^{- 1} \mathbf{C} \mathbf{S}^{-1}\\
    - \mathbf{b} + \mu \mathcal{A} \mathbf{S}^{- 1} \mathbf{C} \mathbf{S}^{- 1} & - \mu (
    \langle \mathbf{C}, \mathbf{S}^{- 1} \mathbf{C} \mathbf{S}^{- 1} \rangle + \tau^{- 2} )
  \end{array}\Bigg) \left(\begin{array}{c}
    \Delta \mathbf{y}\\
    \Delta \tau
  \end{array}\right) =  \left(\begin{array}{c}
    \mathbf{b} \tau\\
    \mathbf{b}^{\top} \mathbf{y} - \mu \tau^{-1}
  \end{array}\right) - \mu \left(\begin{array}{c}
    \mathcal{A} \mathbf{S}^{- 1}\\
    \langle \mathbf{C}, \mathbf{S}^{- 1} \rangle
  \end{array}\right) + \mu \left(\begin{array}{c}
    \mathcal{A} \mathbf{S}^{- 1} \mathbf{R} \mathbf{S}^{- 1}\\
    \langle \mathbf{C}, \mathbf{S}^{- 1} \mathbf{R} \mathbf{S}^{- 1} \rangle
  \end{array}\right) \\ \nonumber
\end{eqnarray*}

In practice, {{\texttt{HDSDP}}} solves $\Delta \mathbf{y}_1 := \mathbf{M}^{- 1} \mathbf{b},
\Delta \mathbf{y}_2 := \mathbf{M}^{- 1} \mathcal{A} \mathbf{S}^{- 1}, \Delta \mathbf{y}_3 := \mathbf{M}^{- 1} \mathcal{A}
\mathbf{S}^{- 1} \mathbf{R} \mathbf{S}^{- 1}, \Delta \mathbf{y}_4 = \mathbf{M}^{- 1} \mathcal{A} \mathbf{S}^{- 1} \mathbf{C} \mathbf{S}^{-
1}$, plugs $\Delta \mathbf{y} = \frac{\tau + \Delta \tau}{\mu} \Delta \mathbf{y}_1 - \Delta
\mathbf{y}_2 + \Delta \mathbf{y}_3 + \Delta \mathbf{y}_4 \Delta \tau$ to get $\Delta \tau$ and finally assembles $\Delta \mathbf{y}$.

 With the above relations, $\mathbf{X} (\mu) := \mu \mathbf{S}^{- 1}
( \mathbf{S} - \mathbf{R} +\mathcal{A}^{\ast} \Delta \mathbf{y} - \mathbf{C} \Delta \tau ) \mathbf{S}^{-
1}$ satisfies $\mathcal{A} \mathbf{X} (\mu) - \mathbf{b} (\tau + \Delta \tau) = \textbf{0}$ and $\mathbf{X} (\mu)
\succeq \textbf{0}$ if and only if the backward Newton step $-\mathcal{A}^{\ast} ( \mathbf{y} - \Delta \mathbf{y} ) + \mathbf{C} (\tau
- \Delta \tau) - \mathbf{R} \succeq \textbf{0}$. When $-\mathcal{A}^{\ast} ( \mathbf{y} -
\Delta \mathbf{y} ) + \mathbf{C} (\tau - \Delta \tau) - \mathbf{R}$ is positive definite, an
objective follows from simple algebraic manupulation
\begin{align*}
\bar{z}  ={} & \langle \mathbf{C} \tau, \mathbf{X} (\mu) \rangle\\
	  ={} & \langle \mathbf{R} + \mathbf{S} + \mathcal{A}^{\ast} \mathbf{y}, \mu \mathbf{S}^{- 1} ( \mathbf{S} - \mathbf{R}
  +\mathcal{A}^{\ast} \Delta \mathbf{y} - \mathbf{C} \Delta \tau ) \mathbf{S}^{- 1}
  \rangle\\
  ={} & (\tau + \Delta \tau) \mathbf{b}^{\top} \mathbf{y} + n \mu + ( \mathcal{A} \mathbf{S}^{- 1} +
  \mathcal{A} \mathbf{S}^{- 1} \mathbf{R} \mathbf{S}^{- 1} )^{\top} \Delta \mathbf{y} + \mu (
  \langle \mathbf{C}, \mathbf{S}^{- 1} \rangle + \langle \mathbf{C}, \mathbf{S}^{- 1} \mathbf{C}
  \mathbf{S}^{- 1} \rangle ) \Delta \tau
\end{align*}
and dividing both sides by $\tau$. Alternatively, {{\texttt{HDSDP}}} extracts a primal
objective bound from the projection problem
\[   \min_{\mathbf{X}} ~ \| \mathbf{S}^{1 / 2} \mathbf{X} \mathbf{S}^{1 / 2} - \mu \mathbf{I} \|_F^2 \quad \text{subject to} \quad \mathcal{A} \mathbf{X} = \mathbf{b} \tau \]

whose optimal solution is $\mathbf{X}' (\mu) := \mu \mathbf{S}^{- 1} ( \mathbf{C} \tau -
\mathcal{A}^{\ast} ( \mathbf{y} - \Delta' \mathbf{y} ) - \mathbf{R} ) \mathbf{S}^{- 1}$, where
$\Delta \mathbf{y}' = \frac{\tau}{\mu} \Delta \mathbf{y}_1 - \Delta \mathbf{y}_2$ and
\[ z' = \langle \mathbf{C} \tau, \mathbf{X}' (\mu) \rangle = \mu \{
   \langle \mathbf{R}, \mathbf{S}^{- 1} \rangle + ( \mathcal{A} \mathbf{S}^{- 1} \mathbf{R}}
   \mathbf{S}^{- 1} + \mathcal{A} \mathbf{S}^{- 1} )^{\top} \Delta' \mathbf{y} + n \} + \tau
   \mathbf{b}^{\top} \mathbf{y} . \]
One major computationally intensive part in {{\texttt{HDSDP}}} is the setup of
the Schur complement matrix $\mathbf{M}$. Since the objective $\mathbf{C}$ often does not
enjoy the same structure as $\{ \mathbf{A}_i \}$, the additional cost from
$\mathcal{A} \mathbf{S}^{- 1} \mathbf{C} \mathbf{S}^{- 1}$ and $\langle \mathbf{C}, \mathbf{S}^{- 1} \mathbf{C} \mathbf{S}^{- 1}
\rangle$ calls for more efficient techniques to set up $\mathbf{M}$. In
{{\texttt{HDSDP}}}, a permutation $\sigma (m)$ of the rows of $\mathbf{M}$ is
generated heuristically to reduce the flops to set up $\mathbf{M}$ row by row.
\[ \left(\begin{array}{ccc}
     \langle \mathbf{A}_{\sigma (1)}, \mathbf{S}^{- 1} \mathbf{A}_{\sigma (1)} \mathbf{S}^{- 1}
     \rangle & \cdots & \langle \mathbf{A}_{\sigma (1)}, \mathbf{S}^{- 1}
     \mathbf{A}_{\sigma (m)} \mathbf{S}^{- 1} \rangle\\
     & \ddots & \vdots\\
     &  & \langle \mathbf{A}_{\sigma (m)}, \mathbf{S}^{- 1} \mathbf{A}_{\sigma (m)} \mathbf{S}^{- 1}
     \rangle
   \end{array}\right) \]
   
Technique {\textbf{M1}} and {\textbf{M2}} are inherited from
{{\texttt{DSDP}}}. They exploit the low-rank structure of the problem data and
an eigen-decomposition of the problem data
$\mathbf{A}_i = \sum_{r = 1}^{\ensuremath{\operatorname{rank}} \left( \mathbf{A}_i \right)} \lambda_{i r}
   \mathbf{a}_{i, r} \mathbf{a}^{\top}_{i, r}$
needs to be computed at the beginning of the algorithm.\\


{	\begin{minipage}{0.8\textwidth}
		{\textbf{KKT Technique M1}}
\begin{enumerate}
  \item {\textbf{Setup}} $\mathbf{B}_{\sigma (i)} = \sum_{r =
  1}^{\ensuremath{\operatorname{rank}} ( \mathbf{A}_{\sigma (i)} )} \lambda_r \big( \mathbf{S}^{- 1}
  \mathbf{a}_{\sigma (i), r} \big) ( \mathbf{S}^{- 1} \mathbf{a}_{\sigma (i), r})^{\top}$.
  
  \item {\textbf{Compute}} $M_{\sigma (i) \sigma (j)} = \langle
  \mathbf{B}_{\sigma (i)}, \mathbf{A}_{\sigma (j)} \rangle, \text{for all } j \geq i$.
\end{enumerate}
{\textbf{KKT Technique M2}}
\begin{enumerate}
  \item {\textbf{Setup}} $\mathbf{S}^{- 1} \mathbf{a}_{\sigma (i), r}, r = 1, \ldots,
  r_{\sigma (i)}$.
  \item {\textbf{Compute}} $M_{\sigma (i) \sigma (j)} = \sum_{r =
  1}^{\ensuremath{\operatorname{rank}} ( \mathbf{A}_{\sigma (i)} )} \big( \mathbf{S}^{- 1}
  \mathbf{a}_{\sigma (i), r} )^{\top} \mathbf{A}_{\sigma (j)} ( \mathbf{S}^{- 1}
  \mathbf{a}_{\sigma (i), r} \big)$.\\
\end{enumerate}
\end{minipage}} 

Technique {\textbf{M3}}, {\textbf{M4}} and {\textbf{M5}} exploit sparsity and need evaluation of $\mathbf{S}^{-1}$ {\cite{fujisawa1997exploiting}}.\\

	{\begin{minipage}{0.8\textwidth}
{\textbf{KKT Technique M3}}
\begin{enumerate}
  \item {\textbf{Setup}} $\mathbf{B}_{\sigma (i)} = \mathbf{S}^{- 1} \mathbf{A}_{\sigma
  (i)} \mathbf{S}^{- 1}$
  \item {\textbf{Compute}} $M_{\sigma (i) \sigma (j)} = \langle
  \mathbf{B}_{\sigma (i)}, \mathbf{A}_{\sigma (j)} \rangle, \text{for all } j \geq i$
\end{enumerate}

{\textbf{KKT Technique M4}}
\begin{enumerate}
  \item {\textbf{Setup}} $\mathbf{B}_{\sigma (i)} = \mathbf{S}^{- 1} \mathbf{A}_{\sigma
  (i)}$
  
  \item {\textbf{Compute}} $M_{\sigma (i) \sigma (j)} = \left\langle
  \mathbf{B}_{\sigma (i)} \mathbf{S}^{- 1}, \mathbf{A}_{\sigma (j)} \right\rangle, \text{for all } j
  \geq i$
\end{enumerate}
{\textbf{KKT Technique M5}}
\begin{enumerate}
  \item {\textbf{Compute}} $M_{\sigma (i) \sigma (j)} = \langle \mathbf{S}^{-
  1} \mathbf{A}_{\sigma (i)} \mathbf{S}^{- 1}, \mathbf{A}_{\sigma (j)} \rangle, \text{for all } j \geq
  i$ directly.\\
\end{enumerate}
\end{minipage}}

\noindent\rule{\textwidth}{0.5pt}

At the beginning of the algorithm, {{\texttt{HDSDP}}} estimates the
number of flops using each technique and each row of $\mathbf{M}$ is associated with the cheapest
technique to minimize the overall computation cost. This is a major
improvement over {{\texttt{DSDP5.8}}} in the computational aspect and 
we leave the details to the later sections. \\

After setting up $\mathbf{M}$, conjugate gradient (CG) method is
employed to solve the linear systems. The maximum number of iterations is
chosen around $50 / m$ and is heuristically adjusted. Either the diagonal of
$\mathbf{M}$ or its Cholesky decomposition is chosen as pre-conditioner and
after a Cholesky pre-conditioner is computed, {{\texttt{HDSDP}}} reuses it for 
the consecutive solves till a heuristic determines that the current
pre-conditioner is outdated. When the algorithm approaches optimality, $\mathbf{M}$
might become ill-conditioned and {{\texttt{HDSDP}}} switches to {\textsf{LDLT}} decomposition in case Cholesky fails.

Using the Newton's direction, {{\texttt{HDSDP}}} computes the maximum stepsize
$$\alpha = \max \left\{ \alpha \in [0, 1] : \mathbf{S} + \alpha \Delta \mathbf{S} \succeq \textbf{0},
\tau + \alpha \Delta \tau \geq 0 \right\}$$ via a Lanczos procedure
{\cite{toh2002note}}. Then to determine a proper stepsize, one line-search
determines $\alpha_c$ such that the barrier satisfies $- \log \det \left( \mathbf{S}
+ \alpha_c \Delta \mathbf{S} \right) - \log \det (\tau + \alpha_c \Delta \tau) \leq -
\log \det \left( \mathbf{S} + \alpha_c \Delta \mathbf{S} \right) - \log \det (\tau +
\alpha_c \Delta \tau)$ and a second line-search chooses $\gamma$ such that $\mathbf{S}
+ \alpha_c \mathcal{A}^{\ast} \Delta \mathbf{y}_2 + \alpha_c \gamma \left( \mathbf{R}
-\mathcal{A}^{\ast} \Delta \mathbf{y}_3 \right) \succeq \textbf{0}$. Next a full direction
$\left( \Delta \mathbf{y}^{\gamma}, \Delta \mathbf{S}^{\gamma}, \Delta \tau^{\gamma} \right)$
is assembled from the Newton system with damping factor $\gamma$ and
a third Lanczos procedure computes  \[\alpha' = \max \left\{ \alpha \in [0, 1] :
\mathbf{S} + \alpha \Delta \mathbf{S}^r \succeq \textbf{0}, \tau + \alpha \Delta \tau^r \geq 0
\right\}\]. Last {{\texttt{HDSDP}}} updates $\mathbf{y} \leftarrow \mathbf{y} + 0.95 \alpha'
\Delta \mathbf{y}^r$ and $\tau \leftarrow \tau + 0.95 \alpha \Delta \tau^r$. To make
further use of the Schur matrix and to maintain centrality, the
above procedure is repeated several times in an iteration. \\

In {{\texttt{HDSDP}}}, the update of the barrier parameter $\mu$ is another
critical factor. At the end of each iteration, {{\texttt{HDSDP}}} updates the
barrier parameter $\mu$ by $\left( \bar{z} - \mathbf{b}^{\top} \mathbf{y} + \theta \left\|
\mathbf{R} \right\|_F \right) / \rho n$, where $\rho$ and $\theta$ are pre-defined
parameters. Heuristics also adjust $\mu$ using  previously computed $\alpha_c, \alpha'$ and $\gamma$. \\

To get the best of the two worlds, {{\texttt{HDSDP}}} implements the same
dual-scaling algorithm as in \text{{\ttfamily{DSDP5.8}}} assuming a dual feasible solution. 
When the dual infeasibility
$\left\| \mathcal{A}^{\ast} \mathbf{y} + \mathbf{S} - \mathbf{C} \right\|_F \leq \varepsilon \tau$ and $\mu$
are sufficiently small, {{\texttt{HDSDP}}} fixes $\tau = 1$, re-starts with
$\left( \mathbf{y} / \tau, \mathbf{S} / \tau \right)$ and applies dual-scaling to guide
convergence. For the detailed implementation of dual-scaling in
\text{{\ttfamily{DSDP5.8}}} refer to {\cite{benson2008algorithm}}. To sum up,
{{\texttt{HDSDP}}} implements strategies and computational tricks tailored for
the embedding and can switch to {{\texttt{DSDP5.8}}} once a dual feasible solution is available.
