\section{Computational results}

The efficiency of robustness of {{\texttt{DSDP5.8}}} has been proven through
years of computational experience and {{\texttt{HDSDP}}} aims to achieve
further improvement on a special class of SDPs where dual method has
advantage over the primal-dual solvers. In this section, we introduce several
classes of SDPs suitable for the dual method and we compare the performance
of {{\texttt{HDSDP}}}, {{\texttt{DSDP5.8}}} and \text{{\ttfamily{COPT 5.0}}} (fastest solver on Mittelmann's benchmark implementing both primal-dual and dual method) on
several benchmark datasets to verify the performance improvement of
{{\texttt{HDSDP}}}. For each problem, we only describe the mathematical model
and refer the readers to
{\cite{mittelmann2003independent,borchers1999sdplib}} for the detailed
background and formulation. The time statistics in this section are obtained
using \text{{\ttfamily{Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz}}} with
\text{{\ttfamily{64GB}}} memory.

\subsection{Maximum-cut}

The SDP relaxation of the max-cut problem is represented by
\begin{eqnarray*}
  \min_{\mathbf{X}} & \left\langle \mathbf{C}, \mathbf{X} \right\rangle & \\
  \text{subject to} & \ensuremath{\operatorname{diag}} \left( \mathbf{X} \right) = \textbf{1} & \\
  & \mathbf{X} \succeq \textbf{0} . & 
\end{eqnarray*}
Let $\mathbf{e}_i$ be the $i$-th column of the identity matrix and the constraint
$\ensuremath{\operatorname{diag}} \left( \mathbf{X} \right) = \mathbf{e}$ is decomposed into $\left\langle \mathbf{X}, \mathbf{e}_i
\mathbf{e}_i^{\top} \right\rangle = 1, i = 1, \ldots, n$. Note that $\mathbf{e}_i \mathbf{e}_i^{\top}$
is rank-one and has only one non-zero entry, {\textbf{M2}} and
{\textbf{M5}} can greatly reduce the computation of the Schur matrix.

\begin{table}[h]
  \begin{tabular}{c|c|c|c|c|c|c|c}
    \hline
    Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT
    v5.0}}} & Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} &
    \text{{\ttfamily{COPT v5.0}}}\\
    \hline
    \text{{\ttfamily{mcp100}}} & \textcolor{red}{0.03} & \textcolor{red}{0.03} & 0.12 & \text{{\ttfamily{maxG51}}} & \textcolor{red}{1.46} &
    4.65 & 5.97\\
    \text{{\ttfamily{mcp124-1}}} & 0.05 & \textcolor{red}{0.03} & 0.15 & \text{{\ttfamily{maxG55}}} & \textcolor{red}{146.92}
    & 606.66 & 520.05\\
    \text{{\ttfamily{mcp124-2}}} & 0.05 & \textcolor{red}{0.02} & 0.17 & \text{{\ttfamily{maxG60}}} & \textcolor{red}{323.62}
    & 1485.00 & 1269.83\\
    \text{{\ttfamily{mcp124-3}}} & 0.05 & \textcolor{red}{0.04} & 0.16 & \text{{\ttfamily{G40\_mb}}} & \textcolor{red}{12.76}
    & 17.08 & 25.76\\
    \text{{\ttfamily{mcp124-4}}} & 0.06 & \textcolor{red}{0.05} & 0.15 & \text{{\ttfamily{G40\_mc}}} & \textcolor{red}{8.09} &
    38.54 & 49.07\\
    \text{{\ttfamily{mcp250-1}}} & 0.15 & \textcolor{red}{0.10} & 0.66 & \text{{\ttfamily{G48\_mb}}} & \textcolor{red}{16.70}
    & 29.71 & 50.49\\
    \text{{\ttfamily{mcp250-2}}} & \textcolor{red}{0.11} & 0.15 & 0.66 & \text{{\ttfamily{G48mc}}} & \textcolor{red}{4.42} &
    18.10 & 33.19\\
    \text{{\ttfamily{mcp250-3}}} & \textcolor{red}{0.17} & 0.21 & 0.62 & \text{{\ttfamily{G55mc}}} & \textcolor{red}{118.11} &
    344.80 & 505.18\\
    \text{{\ttfamily{mcp250-4}}} & \textcolor{red}{0.19} & 0.29 & 0.69 & \text{{\ttfamily{G59mc}}} & \textcolor{red}{181.20} &
    774.70 & 727.89\\
    \text{{\ttfamily{mcp500-1}}} & 0.60 & \textcolor{red}{0.32} & 0.48 & \text{{\ttfamily{G60\_mb}}} & \textcolor{red}{261.50}
    & 650.00 & 964.20\\
    \text{{\ttfamily{mcp500-2}}} & \textcolor{red}{0.69} & 0.74 & 0.72 & \text{{\ttfamily{G60mc}}} & \textcolor{red}{257.10} &
    600.08 & 962.79\\
    \text{{\ttfamily{mcp500-3}}} & \textcolor{red}{0.82} & 1.12 & 1.11 & \text{{\ttfamily{torusg3-8}}} & \textcolor{red}{0.85}
    & 1.42 & 1.04\\
    \text{{\ttfamily{mcp500-4}}} & \textcolor{red}{1.11} & 1.84 & 2.35 & \text{{\ttfamily{torusg3-15}}} &
    \textcolor{red}{23.77} & 178.8 & 137.60\\
    \text{{\ttfamily{maxG11}}} & 1.27 & \textcolor{red}{0.82} & 1.07 & \text{{\ttfamily{toruspm3-8-50}}} &
    \textcolor{red}{0.76} & 0.93 & 0.76\\
    \text{{\ttfamily{maxG32}}} & \textcolor{red}{5.13} & 8.57 & 10.77 & \text{{\ttfamily{toruspm3-15-50}}} &
    \textcolor{red}{19.27} & 91.67 & 117.92\\
    \hline
  \end{tabular}
  \caption{Max-cut problems}
\end{table}

Computational experience suggests that on large-scale max-cut instances, {{\texttt{HDSDP}}} 
is $2\sim3$ times faster than {{\texttt{DSDP5.8}}}.

\subsection{Graph Partitioning}

The SDP relaxation of the graph partitioning problem is given by
\begin{eqnarray*}
  \min_{\mathbf{X}} & \left\langle \mathbf{C}, \mathbf{X} \right\rangle & \\
  \text{subject to} & \ensuremath{\operatorname{diag}} \left( \mathbf{X} \right) = \textbf{1} & \\
  & \left\langle \textbf{{\textbf{1}}1}^{\top}, \mathbf{X} \right\rangle = \beta &
  \\
  & k \mathbf{X} - \textbf{{\textbf{1}}1}^{\top} \succeq \textbf{0} & \\
  & \mathbf{X} \geq \textbf{0}, & 
\end{eqnarray*}
where $\textbf{1}$ denotes the all-one vector and $k, \beta$ are the problem
parameters. Although the dual $\mathbf{S}$ no longer enjoys sparsity, the low-rank
structure is still available to accelerate convergence.

\begin{table}[h]
  \begin{tabular}{c|c|c|c|c|c|c|c}
    \hline
    Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT
    v5.0}}} & Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} &
    \text{{\ttfamily{COPT v5.0}}}\\
    \hline
    \text{{\ttfamily{gpp100}}} & \textcolor{red}{0.03} & 0.04 & 0.19 & \text{{\ttfamily{gpp250-4}}} & 0.19 &
    \textcolor{red}{0.16} & 1.21\\
    \text{{\ttfamily{gpp124-1}}} & \textcolor{red}{0.04} & 0.09 & 0.28 & \text{{\ttfamily{gpp500-1}}} & \textcolor{red}{0.60}
    & 0.63 & 0.64\\
    \text{{\ttfamily{gpp124-2}}} & \textcolor{red}{0.05} & \textcolor{red}{0.05} & 0.28 & \text{{\ttfamily{gpp500-2}}} & 0.69
    & \textcolor{red}{0.60} & 0.56\\
    \text{{\ttfamily{gpp124-3}}} & \textcolor{red}{0.05} & \textcolor{red}{0.05} & 0.34 & \text{{\ttfamily{gpp500-3}}} & 0.82
    & \textcolor{red}{0.55} & 0.56\\
    \text{{\ttfamily{gpp124-4}}} & 0.06 & \textcolor{red}{0.05} & 0.36 & \text{{\ttfamily{gpp500-4}}} & {1.11}
    & \textcolor{red}{0.58} & 0.51\\
    \text{{\ttfamily{gpp250-1}}} & \textcolor{red}{0.15} & 0.16 & 1.58 & \text{{\ttfamily{bm1}}} & \textcolor{red}{2.28} &
    2.36 & 1.74\\
    \text{{\ttfamily{gpp250-2}}} & \textcolor{red}{0.11} & 0.15 & 1.33 & \text{{\ttfamily{biomedP}}} & \textcolor{red}{221.2}
    & \text{{\ttfamily{Failed}}} & \text{{\ttfamily{Failed}}}\\
    \text{{\ttfamily{gpp250-3}}} & 0.17 & \textcolor{red}{0.14} & 1.46 & \text{{\ttfamily{industry2}}} &
    \text{{\ttfamily{Failed}}} & \text{{\ttfamily{Failed}}} & \text{{\ttfamily{Failed}}}\\
    \hline
  \end{tabular}
  \caption{Graph partitioning problems}
\end{table}
On graph partitioning, we see that {{\texttt{HDSDP}}} has comparable performance to {{\texttt{DSDP}}} but is more robust on some problems.

\subsection{Optimal Diagonal Pre-conditioning}

The optimal diagonal pre-conditioning problem originates from
{\cite{qu2020diagonal}}, where given a matrix $\mathbf{B} \succ \textbf{0}$, finding a
diagonal matrix $\mathbf{D}$ to minimize the condition number $\kappa \left(
\mathbf{D}^{- 1 / 2} \mathbf{B} \mathbf{D}^{- 1 / 2} \right)$ can be modeled as an
SDP. The formulation for optimal diagonal pre-conditioning is given by
\begin{eqnarray*}
  \max_{\tau, \mathbf{D}} & \tau & \\
  \text{subject to} & \mathbf{D} \preceq \mathbf{B} & \\
  & \tau \mathbf{B} - \mathbf{D} \preceq \textbf{0} . & 
\end{eqnarray*}
Expressing $\mathbf{D} = \sum_i \mathbf{e}_i \mathbf{e}_i^{\top} d_i$, the problem is exactly
in the SDP dual form. If $\mathbf{B}$ is also sparse, the problem can be
efficiently solved using the dual method.
\begin{table}[h]
\centering
  \begin{tabular}{c|c|c|c}
    \hline
    Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT
    v5.0}}}\\
    \hline
    \text{{\ttfamily{diag-bench-500-0.1}}} & \textcolor{red}{6.70} & 7.50 & 6.80\\
    \text{{\ttfamily{diag-bench-1000-0.01}}} & \textcolor{red}{44.50} & 55.20 & 53.90\\
    \text{{\ttfamily{diag-bench-2000-0.05}}} & \textcolor{red}{34.30} & 340.70 & 307.10\\
    \text{{\ttfamily{diag-bench-west0989}}} & \textcolor{red}{6.72} & 113.23 & 108.20\\
    \text{{\ttfamily{diag-bench-DK01R}}} & \textcolor{red}{13.18} & \text{{\ttfamily{Failed}}} &
    \text{{\ttfamily{Failed}}}\\
    \text{{\ttfamily{diag-bench-micromass\_10NN}}} & \textcolor{red}{9.35} & 60.127 & 51.71\\
    \hline
  \end{tabular}
  \caption{Optimal diagonal pre-conditioning problems}
\end{table}
When the matrix $\mathbf{B}$ is large and sparse, 
we see that {{\texttt{HDSDP}}} dominates the performance of {{\texttt{DSDP}}} due to the Schur complement tricks.

\begin{remark}
For optimal pre-conditioning, we start {{\texttt{HDSDP}}} from
  a non-default trivial dual feasible solution $\tau = - 10^6, \mathbf{D} =
  \mathbf{0}$.
\end{remark}

\subsection{Other Problems}

So far {{\texttt{HDSDP}}} is tested and tuned over a large set of benchmarks
including \text{{\ttfamily{SDPLIB}}} {\cite{borchers1999sdplib}} and Hans
Mittelmann's sparse SDP benchmark {\cite{mittelmann2003independent}}. By the time this manuscript is written, {{\texttt{HDSDP}}} is the fourth fastest among all the benchmarked solvers.
\begin{center}
\begin{lstlisting}
			Scaled shifted geometric means of runtimes ("1" is fastest solver)
            			      1    2.30     4.61     2.28     12.3     3.72
			----------------------------------------------------------------
			count of "a"      8       5       17       13        2       11
			solved of 75     74      70       61       69       64       70
			================================================================
			problem        COPT    CSDP     SDPA    SDPT3   SeDuMi    HDSDP
			================================================================
\end{lstlisting}	
\end{center}

We report the solution accuracy and local CPU time of {{\texttt{HDSDP}}} on
Mittlelmann's benchmark in the appendix. Readers can refer to
{\cite{mittelmann2003independent}} for a detailed explanation of the error
measures and the criterion of a successful solve. Among all of 75 problems, 70
are successfully solved; 3 problems fail due to insufficient memory, 1 fails due to failure to 
find a feasible dual solution and 1 fails
 due to error in primal solution recovery. The benchmark test proves the
efficiency and robustness of {{\texttt{HDSDP}}} as a general purpose SDP
solver. Below we present some benchmark datasets with nice structure
for {{\texttt{HDSDP}}}. They enjoy at least one of sparsity and low-rank
structure.

\begin{table}[h]
\centering
  \begin{tabular}{c|c|c|c|c|c}
    \hline
    Instance & Background & Feature & {{\texttt{HDSDP}}} &
    {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT v5.0}}}\\
    \hline
    \text{{\ttfamily{checker\_1.5}}} & unknown & sparse, low-rank & \textcolor{red}{55.38} & 146.80 &
    137.15\\
    \text{{\ttfamily{foot}}} & unknown & sparse, low-rank & \textcolor{red}{25.76} & 23.83 & 262.54\\
    \text{{\ttfamily{hand}}} & unknown & low-rank & \textcolor{red}{5.60} & 5.57 & 50.70\\
    \text{{\ttfamily{ice\_2.0}}} & unknown & low-rank & \textcolor{red}{706.16} & 1106.00 & 1542.96\\
    \text{{\ttfamily{p\_auss2\_3.0}}} & unknown & sparse, low-rank & \textcolor{red}{739.40} & 1066.00
    & 1111.72\\
    \text{{\ttfamily{rendl1\_2000\_1e-6}}} & unknown & low-rank & \textcolor{red}{15.74} & 22.31 &
    231.80\\
    \text{{\ttfamily{trto3}}} & topology design & sparse, low-rank & \textcolor{red}{1.67} & 1.73 &
    3.04\\
    \text{{\ttfamily{trto4}}} & topology design & sparse, low-rank & \textcolor{red}{12.28} & 12.40 &
    30.09\\
    \text{{\ttfamily{trto5}}} & topology design & sparse, low-rank & \textcolor{red}{111.59} & 151.00 &
    233.16\\
    \text{{\ttfamily{sensor\_500b}}} & sensor network localization & sparse, low-rank
    & 86.73 & {37.87} & \textcolor{red}{7.01}\\
    \text{{\ttfamily{sensor\_1000b}}} & sensor network localization & sparse,
    low-rank & 232.05 & {143.32} & \textcolor{red}{32.27}\\
    \hline
  \end{tabular}
  \caption{Feature of several benchmark problems}
\end{table}

\subsection{Summary}
{{\texttt{HDSDP}}} implements data structures and computational techniques specially optimized for \textit{sparse} SDPs with \textit{low-rank} constraint structure. Vast computational experiments suggests that on these problems {{\texttt{HDSDP}}} outperforms both primal-dual and conventional dual solvers in terms of efficiency and robustness on these instances. 


\section{When (not) to use DSDP/HDSDP}

While {{\texttt{HDSDP}}} is designed for general SDPs, it targets the problems
more tractable in the dual form than by the primal-dual methods. This is
the principle for the techniques implemented by {{\texttt{HDSDP}}}.
Here are some rules in mind when deciding whether to use the dual method (or
{{\texttt{HDSDP}}}).
\begin{enumerate}
  \item Does the problem enjoys nice dual structure?
  
  Many combinatorial problems have formulations friendly to the dual methods.
  Some typical features include (aggregated) sparsity and low-rank structure.
  Dual methods effectively exploit these features by iterating in the dual space and
  using efficient computational tricks. If the problem is dense and most
  constraints are full-rank, dual method has no advantage over the primal-dual
  solvers due to {\textbf{1)}} comparable iteration cost to primal-dual
  methods. {\textbf{2)}} more iterations for convergence.
  
  \item Do we need the primal optimal solution or just the optimal value?
  
  For some applications dual method fails to recover a correct primal solution
  due to numerical difficulties. If the optimal value is sufficient, there is
  no problem. But if an accurate primal optimal solution is always necessary,
  it is better to be more careful and to test the recovery procedure in case
  of failure at the last step.
  
  \item Do we need to certificate infeasibility strictly?
  
  One weakness of the dual method is the difficulty in infeasibility
  certificate. Although on the dual side this issue is addressed by
  {{\texttt{HDSDP}}} using the embedding, dual methods still suffer from
  failure to identify primal infeasibility.
  
  \item Is dual-feasibility hard to attain?
  
  The first phase of {{\texttt{HDSDP}}} adopts the infeasible Newton's method
  and focuses on eliminating the dual infeasibility. This principle works well
  if the dual constraints are relatively easy to satisfy, but if this
  condition fails to hold (e.g., empty dual interior), experiments suggest the embedding
  spend a long time deciding feasibility. In this case it is suggested using
  {{\texttt{DSDP5.8}}} or supply an initial dual solution.
\end{enumerate}
To conclude, {{\texttt{HDSDP}}} solves SDPs but it solves {{\em certain\/}}
SDPs {{\em efficiently\/}}.