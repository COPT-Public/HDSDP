\section{Computational results}

\revised{
The efficiency of {{\texttt{DSDP5.8}}} has been proven through
years of computational experience, and {{\texttt{HDSDP}}} aims to achieve
further improvement on instances where dual method has
advantage over the primal-dual method.  In this section, we introduce several types of SDPs suitable for the dual method and we compare the performance of {{\texttt{HDSDP}}}, {{\texttt{DSDP5.8}}} and \text{{\ttfamily{COPT 6.5}}} (fastest solver on Mittelmann's benchmark) on several benchmark datasets to verify the performance improvement of
{{\texttt{HDSDP}}}.}

\subsection{Experiment Setup}

\revised{We configure the experiment as follows
\begin{enumerate}
	\item (Testing platform). The tests are run on an \text{{\ttfamily{intel i11700K}}} PC with 128GB memory and 12 threads.We choose \texttt{HDSDP}, \texttt{DSDP5.8} and \texttt{COPT6.5} are selected as benchmark softwares.
	\item (Compiler and third-party dependency). Both \texttt{HDSDP} and \texttt{DSDP5.8} are compiled using \texttt{icc} with \texttt{-O2} optimization and linked with threaded version intel MKL. Executable of \texttt{COPT} is directly obtained from its official website.
	\item (Threading). We set the number of MKL threads to be 12 for \texttt{DSDP5.8}; the \texttt{Threads} parameter of \texttt{COPT} is also set to 12. To enhance reproducibility \texttt{HDSDP} uses 8 threads.
	\item (Dataset). Datasets are chosen from three sources: 1). SDP benchmark datasets from Hans Mittelmann's website \cite{mittelmann2003independent}; 2). SDP benchmark datasets from \texttt{SDPLIB} \cite{borchers1999sdplib}. 3). Optimal diagonal pre-conditioner SDPs generated according to \cite{qu2020diagonal}.
	\item (Algorithm). Default methods in \texttt{HDSDP}, \texttt{DSDP5.8}, and the primal-dual interior point method implemented in \texttt{COPT} are compared.
	\item (Tolerance). All the feasibility and optimality tolerances are set to $5\times10^{-6}$.
	\item (Solution status). We adopt the broadly accepted DIMACs error to determine if a solution is qualified. According to \citet{mittelmann2003independent}, if any of the DIMACS errors exceeds $10^{-2}$, the solution is considered invalid.
	\item (Performance Metric). We use shifted geometric mean to compare the overall speed between different solvers.
\end{enumerate}}


\subsection{Maximum Cut}

The SDP relaxation of the max-cut problem is represented by
\begin{eqnarray*}
  \min_{\mathbf{X}} & \left\langle \mathbf{C}, \mathbf{X} \right\rangle & \\
  \text{subject to} & \ensuremath{\operatorname{diag}} \left( \mathbf{X} \right) = \textbf{1} & \\
  & \mathbf{X} \succeq \textbf{0} . & 
\end{eqnarray*}
Let $\mathbf{e}_i$ be the $i$-th column of the identity matrix and the constraint
$\ensuremath{\operatorname{diag}} \left( \mathbf{X} \right) = \mathbf{e}$ is decomposed into $\left\langle \mathbf{X}, \mathbf{e}_i
\mathbf{e}_i^{\top} \right\rangle = 1, i = 1, \ldots, n$. Note that $\mathbf{e}_i \mathbf{e}_i^{\top}$
is rank-one and has only one non-zero entry, {\textbf{M2}} and
{\textbf{M5}} can greatly reduce the computation of the Schur matrix.

\begin{table}[h]
  \caption{Max-cut problems}
  \begin{tabular}{crrrcrrr}
    \toprule
    Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT
    v6.5}}} & Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} &
    \text{{\ttfamily{COPT v6.5}}}\\
    \midrule
    \text{{\ttfamily{mcp100}}} & {0.03} & {0.02} & 0.11 & \text{{\ttfamily{maxG51}}} & {1.45} &
    2.52 & 14.40 \\
    \text{{\ttfamily{mcp124-1}}} & 0.04 & {0.02} & 0.17 & \text{{\ttfamily{maxG55}}} & {38.21}
    & 273.60 & 1096.02 \\
    \text{{\ttfamily{mcp124-2}}} & 0.04 & {0.02} & 0.17 & \text{{\ttfamily{maxG60}}} & {87.59}
    & 535.20 & 2926.11 \\
    \text{{\ttfamily{mcp124-3}}} & 0.05 & {0.02} & 0.16 & \text{{\ttfamily{G40\_mb}}} & {15.77}
    & 8.77 & 98.97\\
    \text{{\ttfamily{mcp124-4}}} & 0.06 & {0.05} & 0.17 & \text{{\ttfamily{G40\_mc}}} & {6.53} &
    18.68 & 80.45\\
    \text{{\ttfamily{mcp250-1}}} & 0.09 & {0.08} & 0.70 & \text{{\ttfamily{G48\_mb}}} & {17.83}
    & 12.48 & 183.95\\
    \text{{\ttfamily{mcp250-2}}} & {0.08} & 0.09 & 0.72 & \text{{\ttfamily{G48mc}}} & {5.09} &
    8.43 & 118.79\\
    \text{{\ttfamily{mcp250-3}}} & {0.09} & 0.12 & 0.65 & \text{{\ttfamily{G55mc}}} & {38.06} &
    168.1 & 1026.10\\
    \text{{\ttfamily{mcp250-4}}} & {0.19} & 0.16 & 0.71 & \text{{\ttfamily{G59mc}}} & {48.73} &
    302.3 & 1131.24\\
    \text{{\ttfamily{mcp500-1}}} & 0.26 & {0.21} & 2.78 & \text{{\ttfamily{G60\_mb}}} & {211.22}
    & 213.4 & 5188.11\\
    \text{{\ttfamily{mcp500-2}}} & {0.25} & 0.32 & 2.82 & \text{{\ttfamily{G60mc}}} & {208.96} &
    218.8 & 5189.29 \\
    \text{{\ttfamily{mcp500-3}}} & {0.30} & 0.50 & 2.59 & \text{{\ttfamily{torusg3-8}}} & {0.50}
    & 0.77 & 1.04\\
    \text{{\ttfamily{mcp500-4}}} & {0.39} & 0.86 & 2.86 & \text{{\ttfamily{torusg3-15}}} &
    {15.30} & 178.8 & 137.60\\
    \text{{\ttfamily{maxG11}}} & 0.60 & {0.46} & 7.15 & \text{{\ttfamily{toruspm3-8-50}}} &
    {0.41} & 0.40 & 2.65\\
    \text{{\ttfamily{maxG32}}} & {4.68} & 3.66 & 68.32 & \text{{\ttfamily{toruspm3-15-50}}} &
    {13.32} & 43.67 & 309.25 \\
    \bottomrule
  \end{tabular}
\end{table}

Computational experience suggests that on large-scale sparse max-cut instances, {{\texttt{HDSDP}}} 
is more than $5$ times faster than {{\texttt{DSDP5.8}}}. Two exceptions are \text{{\ttfamily{G60mc}}} and \text{{\ttfamily{G60\_mb}}}, where the aggregated sparsity pattern of the dual matrix is lost due to the dense objective coefficient, and thereby the dense MKL routines take up most of the computation.

\subsection{Graph Partitioning}

The SDP relaxation of the graph partitioning problem is given by
\begin{eqnarray*}
  \min_{\mathbf{X}} & \langle \mathbf{C}, \mathbf{X} \rangle & \\
  \text{subject to} & \ensuremath{\operatorname{diag}} \left( \mathbf{X} \right) = \textbf{1} & \\
  & \langle \textbf{{\textbf{1}}1}^{\top}, \mathbf{X} \rangle = \beta &
  \\
  & k \mathbf{X} - \textbf{{\textbf{1}}1}^{\top} \succeq \textbf{0} & \\
  & \mathbf{X} \geq \textbf{0}, & 
\end{eqnarray*}
where $\textbf{1}$ denotes the all-one vector and $k, \beta$ are the problem
parameters. Although the dual $\mathbf{S}$ no longer enjoys sparsity, the low-rank
structure is still available to accelerate convergence.

\begin{table}[h]
  \caption{Graph partitioning problems}
  \begin{tabular}{crrrcrrr}
    \toprule
    Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT
    v6.5}}} & Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} &
    \text{{\ttfamily{COPT v6.5}}}\\
    \midrule
    \text{{\ttfamily{gpp100}}} & {0.05} & 0.05 & 0.19 & \text{{\ttfamily{gpp250-4}}} & 0.15 &
    {0.14} & 1.52\\
    \text{{\ttfamily{gpp124-1}}} & {0.07} & 0.07 & 0.33 & \text{{\ttfamily{gpp500-1}}} & {0.89}
    & 0.44 & 5.52\\
    \text{{\ttfamily{gpp124-2}}} & {0.06} & Failed & 0.30 & \text{{\ttfamily{gpp500-2}}} & 0.80
    & {0.43} & 5.43\\
    \text{{\ttfamily{gpp124-3}}} & {0.06} & {0.06} & 0.31 & \text{{\ttfamily{gpp500-3}}} & 0.65
    & {0.36} & 5.54\\
    \text{{\ttfamily{gpp124-4}}} & 0.06 & {0.06} & 0.29 & \text{{\ttfamily{gpp500-4}}} & {0.68}
    & {0.38} & 5.40\\
    \text{{\ttfamily{gpp250-1}}} & {0.20} & Failed & 1.65 & \text{{\ttfamily{bm1}}} & {2.28} &
    1.18 & 18.45 \\
    \text{{\ttfamily{gpp250-2}}} & {0.17} & 0.14 & 1.21 & \text{{\ttfamily{biomedP}}} & {171.64}
    & \text{{\ttfamily{Failed}}} & \text{{\ttfamily{Failed}}}\\
    \text{{\ttfamily{gpp250-3}}} & 0.17 & {0.13} & 1.41 & \\
    \bottomrule
  \end{tabular}
\end{table}
On graph partitioning instances, we see that {{\texttt{HDSDP}}} has comparable performance to {{\texttt{DSDP}}} but is more robust on some of the problems.

\subsection{Optimal Diagonal Pre-conditioning}

The optimal diagonal pre-conditioning problem originates from
{\cite{qu2020diagonal}}, where given a matrix $\mathbf{B} \succ \textbf{0}$, finding a
diagonal matrix $\mathbf{D}$ to minimize the condition number $\kappa (
\mathbf{D}^{- 1 / 2} \mathbf{B} \mathbf{D}^{- 1 / 2} )$ can be modeled as an
SDP. The formulation for optimal diagonal pre-conditioning is given by
\begin{eqnarray*}
  \max_{\tau, \mathbf{D}} & \tau & \\
  \text{subject to} & \mathbf{D} \preceq \mathbf{B} & \\
  & \tau \mathbf{B} - \mathbf{D} \preceq \textbf{0} . & 
\end{eqnarray*}
Expressing $\mathbf{D} = \sum_i \mathbf{e}_i \mathbf{e}_i^{\top} d_i$, the problem is exactly
in the SDP dual form. If $\mathbf{B}$ is also sparse, the problem can be
efficiently solved using the dual method.
\begin{table}[H]
\centering
  \caption{Optimal diagonal pre-conditioning problems}
  \begin{tabular}{rrrr}
    \toprule
    Instance & {{\texttt{HDSDP}}} & {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT
    v6.5}}}\\
    \midrule
    \text{{\ttfamily{diag-bench-1000-0.01}}} & {37.670} & 207.500 & 38.61\\
    \text{{\ttfamily{diag-bench-2000-0.05}}} & 276.960 & 971.700 & {161.17} \\
    \text{{\ttfamily{diag-bench-west0989}}} & {35.280} & \text{{\ttfamily{Failed}}} & 76.900 \\
    \text{{\ttfamily{diag-bench-DK01R}}} & {5.010} & \text{{\ttfamily{Failed}}}  & \text{{\ttfamily{Failed}}} \\
    \text{{\ttfamily{diag-bench-micromass\_10NN}}} & 20.510  &  38.45 & {17.430} \\
    \bottomrule
  \end{tabular}
\end{table}
%When the matrix $\mathbf{B}$ is large and sparse, 
%we see that {{\texttt{HDSDP}}} dominates the performance of {{\texttt{DSDP}}} due to the Schur complement techniques.

%\begin{remark}
%For optimal pre-conditioning, we start {{\texttt{HDSDP}}} from
%  a non-default trivial dual feasible solution $\tau = - 10^6, \mathbf{D} =
%  \mathbf{0}$.
%\end{remark}

\subsection{Other Problems}

So far {{\texttt{HDSDP}}} is tested and tuned over a broad set of benchmarks
including \text{{\ttfamily{SDPLIB}}} {\cite{borchers1999sdplib}} and Hans
Mittelmann's sparse SDP benchmark {\cite{mittelmann2003independent}}. 
Using geometric mean as the metric, compared to {{\texttt{DSDP5.8}}}, {{\texttt{HDSDP}}} achieves a speedup of 21\% on \text{{\ttfamily{SDPLIB}}} and around 17\% speedup on Hans Mittelmann's benchmark dataset. Below we list some example benchmark datasets of nice structure for {{\texttt{HDSDP}}} to exploit.
%By the time this manuscript is written, {{\texttt{HDSDP}}} is the fourth fastest among all the benchmarked solvers.
%\begin{center}
%\begin{lstlisting}
%			Scaled shifted geometric means of runtimes ("1" is fastest solver)
%            			      1    2.30     4.61     2.28     12.3     3.72
%			----------------------------------------------------------------
%			count of "a"      8       5       17       13        2       11
%			solved of 75     74      70       61       69       64       70
%			================================================================
%			problem        COPT    CSDP     SDPA    SDPT3   SeDuMi    HDSDP
%			================================================================
%\end{lstlisting}	
%\end{center}
%
%We report the solution accuracy and local CPU time of {{\texttt{HDSDP}}} on
%Mittlelmann's benchmark in the appendix. Readers can refer to
%{\cite{mittelmann2003independent}} for a detailed explanation of the error
%measures and the criterion of a successful solve. Among all of 75 problems, 70
%are successfully solved; 3 problems fail due to insufficient memory, 1 fails due to failure to 
%find a feasible dual solution and 1 fails
% due to error in primal solution recovery. The benchmark test proves the
%efficiency and robustness of {{\texttt{HDSDP}}} as a general purpose SDP
%solver. 

\begin{table}[h]
\centering
  \caption{Features of several benchmark problems}
  \begin{tabular}{cccrrr}
    \toprule
    Instance & Background & Feature & {{\texttt{HDSDP}}} &
    {{\texttt{DSDP5.8}}} & \text{{\ttfamily{COPT v6.5}}}\\
    \midrule
    \text{{\ttfamily{checker\_1.5}}} & unknown & sparse, low-rank & {39.55} & 64.04 &
    868.37\\
    \text{{\ttfamily{foot}}} & unknown & sparse, low-rank & {26.68} & 13.65 & 245.41\\
    \text{{\ttfamily{hand}}} & unknown & low-rank & {6.60} & 2.64 & 49.39\\
    \text{{\ttfamily{ice\_2.0}}} & unknown & low-rank & {284.90} & 504.70 & 8561.74\\
    \text{{\ttfamily{p\_auss2\_3.0}}} & unknown & sparse, low-rank & {528.00} & 1066.00
    & 1111.72\\
    \text{{\ttfamily{rendl1\_2000\_1e-6}}} & unknown & low-rank & {16.17} & 14.38 &
    111.43\\
    \text{{\ttfamily{trto4}}} & topology design & sparse, low-rank & {6.25} & 6.30 &
    27.16\\
    \text{{\ttfamily{trto5}}} & topology design & sparse, low-rank & {67.22} & 75.20 &
    391.83\\
    \text{{\ttfamily{sensor\_500b}}} & sensor network localization & sparse, low-rank
    & 19.84 & {35.11} & {8.97}\\
    \text{{\ttfamily{sensor\_1000b}}} & sensor network localization & sparse,
    low-rank & 76.39 & {98.18} & {38.96}\\
    \bottomrule
  \end{tabular}
\end{table}

%\subsection{Summary}
%{{\texttt{HDSDP}}} implements data structures and computational techniques specially optimized for \textit{sparse} SDPs with \textit{low-rank} constraint structure. Vast computational experiments suggests that on these problems {{\texttt{HDSDP}}} outperforms both primal-dual and conventional dual solvers in terms of efficiency and robustness on these instances. 

\section{When (not) to use DSDP/HDSDP}

While {{\texttt{HDSDP}}} is designed for general SDPs, it targets the problems
more tractable in the dual form than by the primal-dual methods. This is
the principle for the techniques implemented by {{\texttt{HDSDP}}}.
Here are some rules in mind when deciding whether to use the dual method (or
{{\texttt{HDSDP}}}).
\begin{enumerate}[leftmargin=20pt]
  \item Does the problem enjoys nice dual structure?
  
  Many combinatorial problems have formulations friendly to the dual methods.
  Some typical features include (aggregated) sparsity and low-rank structure.
  Dual methods effectively exploit these features by iterating in the dual space and
  using efficient computational tricks. If the problem is dense and most
  constraints are full-rank, dual method has no advantage over the primal-dual
  solvers due to {\textbf{1)}} comparable iteration cost to primal-dual
  methods. {\textbf{2)}} more iterations for convergence.
  
  \item Do we need the primal optimal solution or just the optimal value?
  
  For some applications dual method fails to recover a correct primal solution
  due to numerical difficulties. If the optimal value is sufficient, there is
  no problem. But if an accurate primal optimal solution is always necessary,
  it is better to be more careful and to test the recovery procedure in case
  of failure at the last step.
  
  \item Do we need to certificate infeasibility strictly?
  
  One weakness of the dual method is the difficulty in infeasibility
  certificate. Although on the dual side this issue is addressed by
  {{\texttt{HDSDP}}} using the embedding, dual methods still suffer from
  failure to identify primal infeasibility.
  
  \item Is dual-feasibility hard to attain?
  
  The first phase of {{\texttt{HDSDP}}} adopts the infeasible Newton's method
  and focuses on eliminating the dual infeasibility. This principle works well
  if the dual constraints are relatively easy to satisfy, but if this
  condition fails to hold (for example, empty dual interior), experiments suggest the embedding
  spend a long time deciding feasibility. In this case it is suggested using
  {{\texttt{DSDP5.8}}} or supply an initial dual solution.
\end{enumerate}