\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DDMP18}

\bibitem[ACCD20]{asi2020minibatch}
Hilal Asi, Karan Chadha, Gary Cheng, and John~C Duchi.
\newblock Minibatch stochastic approximate proximal point methods.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[AD19]{asi2019stochastic}
Hilal Asi and John~C Duchi.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock {\em SIAM Journal on Optimization}, 29(3):2257--2290, 2019.

\bibitem[Ber15]{bertsekas2015convex}
Dimitri Bertsekas.
\newblock {\em Convex optimization algorithms}.
\newblock Athena Scientific, 2015.

\bibitem[CCD{\etalchar{+}}21]{charisopoulos2021low}
Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo D{\'\i}az, Lijun Ding,
  and Dmitriy Drusvyatskiy.
\newblock Low-rank matrix recovery with composite optimization: good
  conditioning and rapid convergence.
\newblock {\em Foundations of Computational Mathematics}, pages 1--89, 2021.

\bibitem[DDC19]{davis2019stochastic}
Damek Davis, Dmitriy Drusvyatskiy, and Vasileios Charisopoulos.
\newblock Stochastic algorithms with geometric step decay converge linearly on
  sharp functions.
\newblock {\em arXiv preprint arXiv:1907.09547}, 2019.

\bibitem[DDMP18]{davis2018subgradient}
Damek Davis, Dmitriy Drusvyatskiy, Kellie~J MacPhee, and Courtney Paquette.
\newblock Subgradient methods for sharp weakly convex functions.
\newblock {\em Journal of Optimization Theory and Applications},
  179(3):962--982, 2018.

\bibitem[DG21]{deng2021minibatch}
Qi~Deng and Wenzhi Gao.
\newblock Minibatch and momentum model-based methods for stochastic non-smooth
  non-convex optimization.
\newblock {\em arXiv preprint arXiv:2106.03034}, 2021.

\end{thebibliography}
