\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,xcolor,latexsym,theorem}
\usepackage{geometry}
\usepackage{multirow}
\usepackage[colorlinks, linkcolor=red, anchorcolor=blue, citecolor=green]{hyperref}

\geometry{a4paper, scale=0.8}
\setlength{\parindent}{0pt}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\assign}{:=}
\newcommand{\cdummy}{\cdot}
\newcommand{\tmcolor}[2]{{\color{#1}{#2}}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmstrong}[1]{\textbf{#1}}
\newcommand{\tmtextit}[1]{\text{{\itshape{#1}}}}
\newcommand{\tmverbatim}[1]{\text{{\ttfamily{#1}}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newtheorem{lemma}{Lemma}
{\theorembodyfont{\rmfamily}\newtheorem{remark}{Remark}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{Stochastic Model-based Algorithm can be Accelerated by Minibatching for
Sharp Functions}

\maketitle

\section{Literature Review}

\begin{table}[h]
\centering

  \begin{tabular}{c|c|c|c|c}
    \hline
    Algorithm & Convexity & Randomness & Stepsize & Complexity\\
    \hline
    \multirow{8}{*}{SGD} & \multirow{4}{*}{Convex} & \multirow{2}{*}{Deterministic} & Constant & $\log (1 / \varepsilon)$ \cite{bertsekas2015convex}\\ \cline{4-5}
    &  &  & Geometrically & $\log (1 / \varepsilon)$ \cite{davis2018subgradient} \\ \cline{3-5}

    &  & \multirow{2}{*}{Stochastic} & Constant & --\\ \cline{4-5}

    &  &  & Geometrically  & $\log (1 / \varepsilon)$ \cite{davis2019stochastic} \\ \cline{2-5}

    & \multirow{4}{*}{Weakly} & \multirow{2}{*}{Deterministic} & Constant & $\log (1 / \varepsilon)$ \cite{davis2018subgradient} \\ \cline{4-5}

    &  &  & Geometrically & $\log (1 / \varepsilon)$ \cite{davis2018subgradient} \\ \cline{3-5}

    &  & \multirow{2}{*}{Stochastic} & Constant & --\\ \cline{4-5}

    &  &  & Geometrically & $\log (1 / \varepsilon)$ \cite{davis2019stochastic}\\ \cline{3-5}
    \hline
    \multirow{8}{*}{SPL/SPP} & \multirow{4}{*}{Convex} & \multirow{2}{*}{Deterministic} & Constant & $\log \log (1 /
    \varepsilon)$ \cite{bertsekas2015convex}\\ \cline{4-5}

    &  &  & Geometrically & \tmcolor{red}{Needed}\\ \cline{3-5}

    &  & \multirow{2}{*}{Stochastic} & Constant & \tmcolor{red}{$\log (1 /
    \varepsilon)^{\dag}$} \cite{asi2019stochastic} \\ \cline{4-5}

    &  &  & Geometrically & $\log (1 / \varepsilon)$ \cite{davis2019stochastic} \\ \cline{2-5}

    & \multirow{4}{*}{Weakly} & \multirow{2}{*}{Deterministic} & Constant & $\log \log (1 / \varepsilon)$ \cite{charisopoulos2021low} \\ \cline{4-5}

    &  &  & Geometrically & \tmcolor{red}{Needed}\\ \cline{3-5}

    &  & \multirow{2}{*}{Stochastic} & Constant & \tmcolor{red}{Needed}\\ \cline{4-5}

    &  &  & Geometrically & $\log (1 / \varepsilon)$ \cite{davis2019stochastic}\\ \cline{3-5}
    \hline
  \end{tabular}
  \caption{Literature over optimization with sharpness}
\end{table}

$\dag$: minibatch acceleration is already proven for easy problems ($\arg
\min_x f (x, \xi) = x^{\ast}, \forall \xi$) in \cite{asi2020minibatch}.

\section{Preliminaries}

Consider the following optimization problem
\begin{eqnarray*}
  \min_{x \in \mathcal{X}} & \mathbb{E}_{\xi} [f (x, \xi)] & 
\end{eqnarray*}
{\tmstrong{Assumption 1}}. It is possible to sample i.i.d. $\{ \xi_1, \ldots,
\xi_n \}$.\\

{\tmstrong{Assumption 2}}. $f$ is $\lambda$-weakly convex. i.e., $f + \frac{\lambda}{2} \| x \|^2$ is convex.\\

{\tmstrong{Assumption 3}}. $f$ is sharp. In other words,
\[ \mu \cdummy \tmop{dist} (x, \mathcal{X}^{\ast}) \leq f (x) - f^{\ast},
   \forall x \in \mathcal{X}^{\ast}, \]
where $\mathcal{X}^{\ast}$ is the set of optimal solutions to the problem.\\

{\tmstrong{Assumption 4}}. $f$ is locally Lipschitz-continuous.

Define the tube $\mathcal{T}_{\gamma} \assign \left\{ x \in \mathcal{X}:
\tmop{dist} (x, \mathcal{X}^{\ast}) \leq \frac{\gamma \mu}{\tau} \right\}$ and
we have
\[ \min_{g \in \partial f_x (x, \xi)} \| g \| \leq L, \forall x \in
   \mathcal{T}_2, \xi . \]
{\tmstrong{Assumption 5}}. Two-sided accuracy is available. i.e.,
\[ | f (y) - f_x (y, \xi) | \leq \frac{\tau}{2} \| x - y \|^2 . \]


\section{Convex Optimization}

To analyze the case of convex optimization, we specially let $\lambda = 0$ and
further assume that global Lipchitzness of the model $f_x (\cdummy, \xi)$
holds.

\subsection{Restarting Strategy with Decaying Stepsize}

\begin{lemma}
  The algorithm in \cite{deng2021minibatch} initialized with $y_0$ satisfies
  \[ \mathbb{E} [f (x^{K + 1}) - f^{\ast}] \leq \frac{2 \tau \tmop{dist}^2
     (y_0, \mathcal{X}^{\ast})}{ (K + 1) (K + 2)} + \frac{4 \sqrt{2} L
     \tmop{dist} (y_0, \mathcal{X}^{\ast})}{\sqrt{3 m (K + 1)}} . \]
\end{lemma}

\begin{lemma}
  For some growth function $g > 0$, denote $E_t \assign \left\{ \tmop{dist}
  (x_t, \mathcal{X}^{\ast}) \leq \frac{R_0}{g (t)} \right\}$ and we have the
  following relation holds
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left[ \frac{2 \tau
    R_0}{\mu K^2} \cdummy \frac{g (t + 1)}{g (t)^2} + \frac{4 \sqrt{6} L}{3
    \sqrt{m (K + 1)}} \cdummy \frac{g (t + 1)}{g (t)} \right] .
  \end{eqnarray*}
\end{lemma}

\begin{proof}
  Without loss of generality we have
  \begin{eqnarray*}
    &  & \mathbb{P} (E_{t + 1})\\
    & = & \mathbb{P} (E_{t + 1} | \overline{E_t}) \mathbb{P} (\overline{E_t})
    +\mathbb{P} (E_t) \mathbb{P} (E_{t + 1} |E_t) \mathbb{P} (E_t)\\
    & \geq & \mathbb{P} (E_t) \mathbb{P} (E_{t + 1} |E_t)
  \end{eqnarray*}
  and that
  \begin{eqnarray*}
    \mathbb{P} (E_{t + 1} |E_t) & = & 1 -\mathbb{P} (\overline{E_{t + 1}}
    |E_t)\\
    & = & 1 -\mathbb{P} \left( \tmop{dist} (x_{t + 1}, \mathcal{X}^{\ast})
    \geq \frac{R_0}{g (t + 1)} |E_t \right)\\
    & \geq & 1 - \frac{\mathbb{E} [\tmop{dist} (x_{t + 1},
    \mathcal{X}^{\ast}) | E_t]}{R_0 / g (t + 1)}\\
    & = & 1 - \frac{\mathbb{E} [\tmop{dist} (x_{t + 1}, \mathcal{X}^{\ast})
    \mathbb{I} \{ E_t \}]}{R_0 / g (t + 1)} \frac{1}{\mathbb{P} (E_t)},
  \end{eqnarray*}
  where the inequality is by Markov's inequality. Then we consider
  \begin{eqnarray*}
    \mathbb{E} [\tmop{dist} (x_{t + 1}, \mathcal{X}^{\ast}) \mathbb{I} \{ E_t
    \}] & \leq & \frac{1}{\mu} \mathbb{E} [(f (x_{t + 1}) - f^{\ast})
    \mathbb{I} \{ E_t \}]\\
    & \leq & \frac{1}{\mu} \left\{ \frac{2 \tau \mathbb{E} [\tmop{dist}^2
    (x_t, \mathcal{X}^{\ast}) \mathbb{I} \{ E_t \}]}{(K + 1) (K + 2)} +
    \frac{4 \sqrt{2} L\mathbb{E} [\tmop{dist} (x_t, \mathcal{X}^{\ast})
    \mathbb{I} \{ E_t \}]}{\sqrt{3 m (K + 1)}} \right\}\\
    & \leq & \frac{2 \tau R_0^2}{\mu K^2} \cdummy \frac{1}{g (t)^2} + \frac{4
    \sqrt{6} L R_0}{3 \sqrt{m (K + 1)}} \cdummy \frac{1}{g (t)} .
  \end{eqnarray*}
  Next we combine the above and obtain that
  \begin{eqnarray*}
    &  & \mathbb{P} (E_{t + 1})\\
    & \geq & \mathbb{P} (E_t) \left\{ 1 - \frac{\mathbb{E} [\tmop{dist} (x_{t
    + 1}, \mathcal{X}^{\ast}) \mathbb{I} \{ E_t \}]}{R_0 / g (t + 1)}
    \frac{1}{\mathbb{P} (E_t)} \right\}\\
    & = & \mathbb{P} (E_t) - \frac{\mathbb{E} [\tmop{dist} (x_{t + 1},
    \mathcal{X}^{\ast}) \mathbb{I} \{ E_t \}]}{R_0 / g (t + 1)}\\
    & \geq & \mathbb{P} (E_t) - \left[ \frac{2 \tau R_0}{\mu K^2} \cdummy
    \frac{g (t + 1)}{g (t)^2} + \frac{4 \sqrt{6} L}{3 \sqrt{m (K + 1)}}
    \cdummy \frac{g (t + 1)}{g (t)} \right] .
  \end{eqnarray*}
  Summing over $t = 0, \ldots, T - 1$ gives
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left[
    \underbrace{\frac{2 \tau R_0}{\mu K^2} \cdummy \frac{g (t + 1)}{g
    (t)^2}}_{\text{Quadratic}} + \underbrace{\frac{4 \sqrt{6} L}{3 \sqrt{m (K
    + 1)}} \cdummy \frac{g (t + 1)}{g (t)}}_{\text{Linear}} \right]
  \end{eqnarray*}
  
\end{proof}

\begin{remark}
  For \tmverbatim{SPP} algorithm we have $\tau = 0$ and the quadratic
  acceleration term is not present and we hence have
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \frac{4 \sqrt{6} L}{3 \sqrt{m (K + 1)}}
    \sum_{t = 0}^{T - 1} \frac{g (t + 1)}{g (t)}
  \end{eqnarray*}
\end{remark}

\begin{remark}
  To recover the deterministic quadratic convergence, we let $m \rightarrow
  \infty$ and get
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \frac{2 \tau R_0}{\mu K^2} \sum_{t = 0}^{T -
    1} \frac{g (t + 1)}{g (t)^2}
  \end{eqnarray*}
  and this allows us to take growth function to $g (t) = 2^{2^t}$ such that
  $\frac{g (t + 1)}{g (t)^2} = 2 =\mathcal{O} (1)$. Then we can follow
  \cite{davis2019stochastic} to recover the quadratic convergence.
\end{remark}

\newpage

From now on we assume that $\tau = 0$ (proximal point) and carry out the analysis.\\
 
\subsection{Optimal Choice for Parameters}

Now we consider the general choice of $g (t), m_t$ and $K_t$. For brevity we
use $m (t)$ and $K (t)$ as functions of discrete values $t$. Then due to
monotonicity of $g$ we have $T = g^{- 1} (t)$ and that
\begin{eqnarray*}
  \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1}
  \left( \frac{8 \sqrt{6} L}{3 \mu \sqrt{K (t) + 1}} \cdummy \frac{g (t +
  1)}{g (t) \sqrt{m (t)}} \right) .
\end{eqnarray*}
Also, we have the total sample complexity given by
\[ \sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1} m (t) K (t) . \]
Then we use $K (t) + 1$ to replace $K (t)$ and get an abstract optimization
problem
\begin{eqnarray*}
  \min_{g, m, K} & &\sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1} m (t) K (t)
  \\
  \text{subject to} & &\sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1} \left(
  \frac{8 \sqrt{6} L}{3 \mu} \cdummy \frac{g (t + 1)}{g (t) \sqrt{m (t) K
  (t)}} \right) \leq \delta.
\end{eqnarray*}
To solve the problem, we first denote $\alpha \assign R_0 / \varepsilon,
\theta \assign \frac{\sqrt{6} \mu \delta}{16 L}, u (t) \assign m (t) K (t)$
and get
\begin{eqnarray*}
  \min_{g, u} & &\sum_{t = 0}^{g^{- 1} (\alpha) - 1} u (t)\\
  \text{subject to} & & \sum_{t = 0}^{g^{- 1} (\alpha) - 1} \frac{1}{\sqrt{u
  (t)}} \cdummy \frac{g (t + 1)}{g (t)} \leq \theta.
\end{eqnarray*}\

{\tmstrong{Linear Convergence}}\\

In this case we have $\frac{g (t + 1)}{g (t)} = \beta$ and by optimality
condition we know that it is optimal to let $u (t_1) = u (t_2), \forall t_1,
t_2$ and the constraint is transformed into
\[ \frac{\log_{\beta} (\alpha)}{\sqrt{u (0)}} \leq \theta / \beta \Rightarrow
   u (0) \geq \frac{\beta^2 \log^2_{\beta} (\alpha)}{\theta^2} = \frac{128 L^2
   \beta^2 \log^2_{\beta} (\alpha)}{3 \mu^2 \delta^2} . \]
Also the objective is into
\[ \sum_{t = 0}^{g^{- 1} (\alpha) - 1} u (t) = \log_{\beta} (\alpha) u (0)
   \geq \left( \frac{\beta}{\log^3 (\beta)} \right) \left( \frac{128 L^2}{3
   \mu^2 \delta^2} \right) \log^3 (\alpha) . \]
Hence the best bound in terms of linear convergence is attained by $\beta =
e^3 \Rightarrow \frac{\beta}{\log^3 (\beta)} = \frac{e^3}{27}$ with constant
batchsize and this gives the best sample complexity
\[ \frac{128 e^3}{81} \left( \frac{L^2}{\mu^2 \delta^2} \right) \log^3 \left(
   \frac{R_0}{\varepsilon} \right) . \]\\
   
{\tmstrong{Super-linear $\exp (t \log (t + 1))$}}

In this case we have $\frac{g (t + 1)}{g (t)} = \left( 1 + \frac{1}{t + 1}
\right)^t (t + 2)$ and in this case we have
\begin{eqnarray*}
  \min_{g, u} & &\sum_{t = 0}^{W (R_0 / \varepsilon) - 1} u (t) \\
  \text{subject to} & &\sum_{t = 0}^{W (e R_0 / \varepsilon) - 2}
  \frac{1}{\sqrt{u (t)}} \cdummy \left( 1 + \frac{1}{t} \right)^t (t + 1) \leq
  \theta ,
\end{eqnarray*}
where $W (x)$ is the Lambert-W function. By taking $m (t) \equiv m, K (t) =
\frac{512 L^2 e^2}{3 m \mu^2 \delta^2} \log^4 \left( \frac{R_0}{\varepsilon}
\right)$ we have the sample complexity of $o \left( \frac{512 L^2}{3 \mu^2
\delta^2} \log^5 \left( \frac{R_0}{\varepsilon} \right) \right)$. Hence we
achieve super-linear convergence.\\

\begin{remark}
	Currently for $\exp (t \log (t + 1))$ we can preserve the order of the $\log\left( \frac{R_0}{\varepsilon} \right)$ term in sample complexity.\\
\end{remark}

   
{\tmstrong{Constant Sample per Iteration}}\\
In this case we assume that $u (t) \equiv u$ and we have
\begin{eqnarray*}
  \min_{g, u} & & g^{- 1} (\alpha)\\
  \text{subject to}& & \sum_{t = 0}^{g^{- 1} (\alpha) - 1} \frac{g (t + 1)}{g
  (t)} \leq \theta \sqrt{u}.
\end{eqnarray*}
Or more abstractly, we have to solve
\begin{eqnarray*}
  \min_f & &f^{- 1} (\alpha) \\
  \text{subject to} & & \int_0^{f^{- 1} (\alpha)} \frac{f (x + 1)}{f (x)} d x
  \leq 1.
\end{eqnarray*}


\bibliography{refsharp.bib}
\bibliographystyle{alpha}


\end{document}
