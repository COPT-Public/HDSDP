\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,xcolor,latexsym,theorem}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\assign}{:=}
\newcommand{\cdummy}{\cdot}
\newcommand{\tmcolor}[2]{{\color{#1}{#2}}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmstrong}[1]{\textbf{#1}}
\newcommand{\tmtextit}[1]{\text{{\itshape{#1}}}}
\newcommand{\tmverbatim}[1]{\text{{\ttfamily{#1}}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newtheorem{lemma}{Lemma}
{\theorembodyfont{\rmfamily}\newtheorem{remark}{Remark}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{Stochastic Model-based Algorithm can be Accelerated by Minibatching for
Sharp Functions}

\maketitle

\section{Literature Review}

\begin{table}[h]
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    Algorithm & Convexity & Randomness & Stepsize & Complexity\\
    \hline
    SGD & Convex & Determinic & Constant & $\log (1 / \varepsilon)$\\
    \hline
    &  &  & Geometrically & $\log (1 / \varepsilon)$\\
    \hline
    &  & Stochastic & Constant & --\\
    \hline
    &  &  & Geometrically  & $\log (1 / \varepsilon)$\\
    \hline
    & Weakly & Deterministic & Constant & $\log (1 / \varepsilon)$\\
    \hline
    &  &  & Geometrically & $\log (1 / \varepsilon)$\\
    \hline
    &  & Stochastic & Constant & --\\
    \hline
    &  &  & Geometrically & $\log (1 / \varepsilon)$\\
    \hline
    SPL/SPP & Convex & Deterministic & Constant & $\log \log (1 /
    \varepsilon)$\\
    \hline
    &  &  & Geometrically & \tmcolor{red}{Needed}\\
    \hline
    &  & Stochastic & Constant & \tmcolor{red}{$\log (1 /
    \varepsilon)^{\dag}$}\\
    \hline
    &  &  & Geometrically & $\log (1 / \varepsilon)$\\
    \hline
    & Weakly & Deterministic & Constant & $\log \log (1 / \varepsilon)$\\
    \hline
    &  &  & Geometrically & \tmcolor{red}{Needed}\\
    \hline
    &  & Stochastic & Constant & \tmcolor{red}{Needed}\\
    \hline
    &  &  & Geometrically & $\log (1 / \varepsilon)$\\
    \hline
  \end{tabular}
  \caption{Literature over optimization with sharpness}
\end{table}

$\dag$: minibatch acceleration is already proven for easy problems ($\arg
\min_x f (x, \xi) = x^{\ast}, \forall \xi$).

\section{Preliminaries}

Consider the following optimization problem
\begin{eqnarray*}
  \min_{x \in \mathcal{X}} & \mathbb{E}_{\xi} [f (x, \xi)] & 
\end{eqnarray*}
{\tmstrong{Assumption 1}}. It is possible to sample i.i.d. $\{ \xi_1, \ldots,
\xi_n \}$.

{\tmstrong{Assumption 2}}. $f$ is $\lambda$-weakly convex.

We assume that $f + \frac{\lambda}{2} \| x \|^2$ is convex.

{\tmstrong{Assumption 3}}. $f$ is sharp. In other words,
\[ \mu \cdummy \tmop{dist} (x, \mathcal{X}^{\ast}) \leq f (x) - f^{\ast},
   \forall x \in \mathcal{X}^{\ast}, \]
where $\mathcal{X}^{\ast}$ is the set of optimal solutions to the problem.

{\tmstrong{Assumption 4}}. $f$ is locally Lipschitz-continuous.

Define the tube $\mathcal{T}_{\gamma} \assign \left\{ x \in \mathcal{X}:
\tmop{dist} (x, \mathcal{X}^{\ast}) \leq \frac{\gamma \mu}{\tau} \right\}$ and
we have
\[ \min_{g \in \partial f_x (x, \xi)} \| g \| \leq L, \forall x \in
   \mathcal{T}_2, \xi . \]
{\tmstrong{Assumption 5}}. Two-sided accuracy is available. i.e.,
\[ | f (y) - f_x (y, \xi) | \leq \frac{\tau}{2} \| x - y \|^2 . \]
It is already known that in the convex case, the proximal point method
converges quadratically [1] and its stochastic variant has linear convergence
when using a geometrically decaying stepsize [2]. Hence there is space for
acceleration.

\section{Convex Optimization}

To analyze the case of convex optimization, we specially let $\lambda = 0$ and
further assume that global Lipschitzness of the model $f_x (\cdummy, \xi)$
holds.

\subsection{Restarting Strategy with Decaying Stepsize}

\begin{lemma}
  The algorithm in [SMOD] initialized with $y_0$ and satisfies
  \[ \mathbb{E} [f (x^{K + 1}) - f^{\ast}] \leq \frac{2 \tau \tmop{dist}^2
     (y_0, \mathcal{X}^{\ast})}{ (K + 1) (K + 2)} + \frac{4 \sqrt{2} L
     \tmop{dist} (y_0, \mathcal{X}^{\ast})}{\sqrt{3 m_t (K + 1)}} . \]
\end{lemma}

\begin{lemma}
  For some growth function $g > 0$, denote $E_t \assign \left\{ \tmop{dist}
  (x_t, \mathcal{X}^{\ast}) \leq \frac{R_0}{g (t)} \right\}$ and we have the
  following relation holds
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left[ \frac{2 \tau
    R_0}{\mu K^2} \cdummy \frac{g (t + 1)}{g (t)^2} + \frac{4 \sqrt{6} L}{3
    \sqrt{m_t  (K + 1)}} \cdummy \frac{g (t + 1)}{g (t)} \right] .
  \end{eqnarray*}
\end{lemma}

\begin{proof}
  Without loss of generality we have
  \begin{eqnarray*}
    &  & \mathbb{P} (E_{t + 1})\\
    & = & \mathbb{P} (E_{t + 1} | \overline{E_t}) \mathbb{P} (\overline{E_t})
    +\mathbb{P} (E_t) \mathbb{P} (E_{t + 1} |E_t) \mathbb{P} (E_t)\\
    & \geq & \mathbb{P} (E_t) \mathbb{P} (E_{t + 1} |E_t)
  \end{eqnarray*}
  and that
  \begin{eqnarray*}
    \mathbb{P} (E_{t + 1} |E_t) & = & 1 -\mathbb{P} (\overline{E_{t + 1}}
    |E_t)\\
    & = & 1 -\mathbb{P} \left( \tmop{dist} (x_{t + 1}, \mathcal{X}^{\ast})
    \geq \frac{R_0}{g (t + 1)} |E_t \right)\\
    & \geq & 1 - \frac{\mathbb{E} [\tmop{dist} (x_{t + 1},
    \mathcal{X}^{\ast}) | E_t]}{R_0 / g (t + 1)}\\
    & = & 1 - \frac{\mathbb{E} [\tmop{dist} (x_{t + 1}, \mathcal{X}^{\ast})
    \mathbb{I} \{ E_t \}]}{R_0 / g (t + 1)} \frac{1}{\mathbb{P} (E_t)},
  \end{eqnarray*}
  where the inequality is by Markov's inequality.
  
  Then we consider
  \begin{eqnarray*}
    \mathbb{E} [\tmop{dist} (x_{t + 1}, \mathcal{X}^{\ast}) \mathbb{I} \{ E_t
    \}] & \leq & \frac{1}{\mu} \mathbb{E} [(f (x_{t + 1}) - f^{\ast})
    \mathbb{I} \{ E_t \}]\\
    & \leq & \frac{1}{\mu} \left\{ \frac{2 \tau \mathbb{E} [\tmop{dist}^2
    (x_t, \mathcal{X}^{\ast}) \mathbb{I} \{ E_t \}]}{(K + 1) (K + 2)} +
    \frac{4 \sqrt{2} L\mathbb{E} [\tmop{dist} (x_t, \mathcal{X}^{\ast})
    \mathbb{I} \{ E_t \}]}{\sqrt{3 m_t (K + 1)}} \right\}\\
    & \leq & \frac{2 \tau R_0^2}{\mu K^2} \cdummy \frac{1}{g (t)^2} + \frac{4
    \sqrt{6} L R_0}{3 \mu \sqrt{m_t  (K + 1)}} \cdummy \frac{1}{g (t)} .
  \end{eqnarray*}
  Next we combine the above and obtain that
  \begin{eqnarray*}
    &  & \mathbb{P} (E_{t + 1})\\
    & \geq & \mathbb{P} (E_t) \left\{ 1 - \frac{\mathbb{E} [\tmop{dist} (x_{t
    + 1}, \mathcal{X}^{\ast}) \mathbb{I} \{ E_t \}]}{R_0 / g (t + 1)}
    \frac{1}{\mathbb{P} (E_t)} \right\}\\
    & = & \mathbb{P} (E_t) - \frac{\mathbb{E} [\tmop{dist} (x_{t + 1},
    \mathcal{X}^{\ast}) \mathbb{I} \{ E_t \}]}{R_0 / g (t + 1)}\\
    & \geq & \mathbb{P} (E_t) - \left[ \frac{2 \tau R_0}{\mu K^2} \cdummy
    \frac{g (t + 1)}{g (t)^2} + \frac{4 \sqrt{6} L}{3 \mu \sqrt{m_t  (K + 1)}}
    \cdummy \frac{g (t + 1)}{g (t)} \right] .
  \end{eqnarray*}
  Summing over $t = 0, \ldots, T - 1$ gives
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left[
    \underbrace{\frac{2 \tau R_0}{\mu K^2} \cdummy \frac{g (t + 1)}{g
    (t)^2}}_{\text{Quadratic}} + \underbrace{\frac{4 \sqrt{6} L}{3 \mu
    \sqrt{m_t (K + 1)}} \cdummy \frac{g (t + 1)}{g (t)}}_{\text{Linear}}
    \right]
  \end{eqnarray*}
  
\end{proof}

\begin{remark}
  For \tmverbatim{SPP} algorithm we have $\tau = 0$ and the quadratic
  acceleration term is not present and we hence have
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \frac{4 \sqrt{6} L}{3 \mu \sqrt{m (K + 1)}}
    \sum_{t = 0}^{T - 1} \frac{g (t + 1)}{g (t)}
  \end{eqnarray*}
\end{remark}

\begin{remark}
  To recover the deterministic quadratic convergence, we let $m \rightarrow
  \infty$ and get
  \begin{eqnarray*}
    \mathbb{P} (E_T) & \geq & 1 - \frac{2 \tau R_0}{\mu K^2} \sum_{t = 0}^{T -
    1} \frac{g (t + 1)}{g (t)^2}
  \end{eqnarray*}
  and this allows us to take growth function to $g (t) = 2^{2^t}$ such that
  $\frac{g (t + 1)}{g (t)^2} = 2 =\mathcal{O} (1)$. Then we can follow
  [{\tmem{Dmitri}}] to recover the quadratic convergence.
\end{remark}

Now we analyze the way to choose $(g, \{ m_t \})$ for faster convergence.

Consider taking $m_t = m (t)$ and we get
\begin{eqnarray*}
  \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left( \frac{2 \tau
  R_0}{\mu K^2} \cdummy \frac{g (t + 1)}{g (t)^2} + \frac{4 \sqrt{6} L}{3 \mu
  \sqrt{K + 1}} \cdummy \frac{g (t + 1)}{\sqrt{m_t} g (t)} \right) .
\end{eqnarray*}
For brevity we first consider the proximal point method with $\tau = 0$ and we
get the bound
\begin{eqnarray*}
  \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left( \frac{4 \sqrt{6}
  L}{3 \mu \sqrt{K_t + 1}} \cdummy \frac{g (t + 1)}{\sqrt{m_t} g (t)} \right)
  .
\end{eqnarray*}
{\tmstrong{Super-linear Batchsize}}

Take $g (t) = 2^{t^2}$ and we have
\begin{eqnarray*}
  \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{T - 1} \left( \frac{8 \sqrt{6}
  L}{3 \mu \sqrt{K_t + 1}} \cdummy \frac{4^t}{\sqrt{m_t}} \right) .
\end{eqnarray*}
Take $m_t = 16^t, T = \left\lceil \sqrt{\log_2 \left( \frac{R_0}{\varepsilon}
\right)} \right\rceil$ and $K_t \equiv \left\lfloor \frac{128 T^2}{3} \cdummy
\left( \frac{L}{\delta \mu} \right)^2 \right\rfloor$, we have the total sample
complexity of
\begin{eqnarray*}
  \sum_{t = 0}^{T - 1} m_t K_t & = & \frac{128 T^2}{3} \left( \frac{L}{\delta
  \mu} \right)^2 \sum_{t = 0}^{T - 1} 16^t\\
  & \leq & \frac{128 T^2}{45} \left( \frac{L}{\delta \mu} \right)^2 \exp
  \left( 4 \sqrt{\log_2 \left( \frac{R_0}{\varepsilon} \right)} \right) \\
  & \leq & \frac{128 \log_2 \left( \frac{R_0}{\varepsilon} \right)}{45}
  \left( \frac{L}{\delta \mu} \right)^2 \exp \left( 4 \sqrt{\log_2 \left(
  \frac{R_0}{\varepsilon} \right)} \right)
\end{eqnarray*}
{\tmstrong{Optimal Choice for Parameters}}

Last we consider the general choice of $g (t), m_t$ and $K_t$. For brevity we
use $m (t)$ and $K (t)$ as functions of discrete values $t$. Then due to
monotonicity of $g$ we have $T = g^{- 1} (t)$ and that
\begin{eqnarray*}
  \mathbb{P} (E_T) & \geq & 1 - \sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1}
  \left( \frac{8 \sqrt{6} L}{3 \mu \sqrt{K (t) + 1}} \cdummy \frac{g (t +
  1)}{g (t) \sqrt{m (t)}} \right) .
\end{eqnarray*}
Also, we have the total sample complexity given by
\[ \sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1} m (t) K (t) . \]
Then we use $K (t) + 1$ to replace $K (t)$ and get an abstract optimization
problem
\begin{eqnarray*}
  \min_{g, m, K} & \sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1} m (t) K (t)
  & \\
  \text{subject to} & \sum_{t = 0}^{g^{- 1} (R_0 / \varepsilon) - 1} \left(
  \frac{8 \sqrt{6} L}{3 \mu} \cdummy \frac{g (t + 1)}{g (t) \sqrt{m (t) K
  (t)}} \right) \leq \delta & .
\end{eqnarray*}
To solve the problem, we first denote $\alpha \assign R_0 / \varepsilon,
\theta \assign \frac{\sqrt{6} \mu \delta}{16 L}, u (t) \assign m (t) K (t)$
and get
\begin{eqnarray*}
  \min_{g, u} & \sum_{t = 0}^{g^{- 1} (\alpha) - 1} u (t) & \\
  \text{subject to} & \sum_{t = 0}^{g^{- 1} (\alpha) - 1} \frac{1}{\sqrt{u
  (t)}} \cdummy \frac{g (t + 1)}{g (t)} \leq \theta & .
\end{eqnarray*}
Now we consider the following cases.

{\tmstrong{Linear Convergence}}

In this case we have $\frac{g (t + 1)}{g (t)} = \beta$ and by optimality
condition we know that it is optimal to let $u (t_1) = u (t_2), \forall t_1,
t_2$ and the constraint is transformed into
\[ \frac{\log_{\beta} (\alpha)}{\sqrt{u (0)}} \leq \theta / \beta \Rightarrow
   u (0) \geq \frac{\beta^2 \log^2_{\beta} (\alpha)}{\theta^2} = \frac{128 L^2
   \beta^2 \log^2_{\beta} (\alpha)}{3 \mu^2 \delta^2} . \]
Also the objective is into
\[ \sum_{t = 0}^{g^{- 1} (\alpha) - 1} u (t) = \log_{\beta} (\alpha) u (0)
   \geq \left( \frac{\beta}{\log^3 (\beta)} \right) \left( \frac{128 L^2}{3
   \mu^2 \delta^2} \right) \log^3 (\alpha) . \]
Hence the best bound in terms of linear convergence is attained by $\beta =
e^3 \Rightarrow \frac{\beta}{\log^3 (\beta)} = \frac{e^3}{27}$ with constant
batchsize and this gives the best sample complexity
\[ \frac{128 e^3}{81} \left( \frac{L^2}{\mu^2 \delta^2} \right) \log^3 \left(
   \frac{R_0}{\varepsilon} \right) . \]
{\tmstrong{Constant Sample per Iteration}}

In this case we assume that $u (t) \equiv u$ and we have
\begin{eqnarray*}
  \min_{g, u} & g^{- 1} (\alpha) & \\
  \text{subject to} & \sum_{t = 0}^{g^{- 1} (\alpha) - 1} \frac{g (t + 1)}{g
  (t)} \leq \theta \sqrt{u} & .
\end{eqnarray*}
Or more abstractly, we have to solve
\begin{eqnarray*}
  \min_f & f^{- 1} (\alpha) & \\
  \text{subject to} & \int_0^{f^{- 1} (\alpha)} \frac{f (x + 1)}{f (x)} d x
  \leq 1 & 
\end{eqnarray*}


{\tmstrong{Super-linear $\exp (t \log (t + 1))$}}

In this case we have $\frac{g (t + 1)}{g (t)} = \left( 1 + \frac{1}{t + 1}
\right)^t (t + 2)$ and in this case we have
\begin{eqnarray*}
  \min_{g, u} & \sum_{t = 0}^{W (R_0 / \varepsilon) - 1} u (t) & \\
  \text{subject to} & \sum_{t = 0}^{W (e R_0 / \varepsilon) - 2}
  \frac{1}{\sqrt{u (t)}} \cdummy \left( 1 + \frac{1}{t} \right)^t (t + 1) \leq
  \theta & ,
\end{eqnarray*}
where $W (x)$ is the Lambert-W function. By taking $m (t) \equiv m, K (t) =
\frac{512 L^2 e^2}{3 m \mu^2 \delta^2} \log^4 \left( \frac{R_0}{\varepsilon}
\right)$ we have the sample complexity of $o \left( \frac{512 L^2}{3 \mu^2
\delta^2} \log^5 \left( \frac{R_0}{\varepsilon} \right) \right)$. Hence we
achieve super-linear convergence.

\

{\tmstrong{Super-linear $\exp (\mathcal{P} (t))$}}

In this case we consider a special case of super-linear convergence with $g
(t) = e^{\beta t^p}$. In this case we have $\frac{g (t + 1)}{g (t)} = \exp
(\beta (t + 1)^p - \beta t^p)$ and $T = g^{- 1} (\alpha) = [\log (\alpha)]^{1
/ p}$. Hence we have the optimization problem given by
\begin{eqnarray*}
  \min_{p, u} & \sum_{t = 0}^{[\log (\alpha)]^{1 / p} - 1} u (t) & \\
  \text{subject to} & \sum_{t = 0}^{[\log (\alpha)]^{1 / p} - 1}
  \frac{1}{\sqrt{u (t)}} \cdummy \exp ((t + 1)^p - t^p) \leq \theta . & 
\end{eqnarray*}
A trivial selection is $p = 2$ and $\frac{g (t + 1)}{g (t)} = \exp (2 \beta t
+ \beta)$. Then we have $[\log (\alpha)]^{1 / p} - 1 = \sqrt{\log (\alpha)} -
1$, giving
\begin{eqnarray*}
  \min_u & \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} u (t) & \\
  \text{subject to} & \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} \frac{\exp (2
  \beta t)}{\sqrt{u (t)}} \leq \theta e^{- \beta} . & 
\end{eqnarray*}
Then by writing the Lagrangian function
\begin{eqnarray*}
  \mathcal{L} (\{ u (t) \}, \lambda) & \assign & \sum_{t = 0}^{\sqrt{\log
  (\alpha)} - 1} u (t) - \lambda \left( \theta e^{- \beta} - \sum_{t =
  0}^{\sqrt{\log (\alpha)} - 1} \frac{\exp (2 \beta t)}{\sqrt{u (t)}} \right)
\end{eqnarray*}
we have
\begin{eqnarray*}
  \partial_{u (t)} \mathcal{L} & = & 1 - \frac{\lambda \exp (2 \beta t)}{2} u
  (t)^{- 3 / 2}
\end{eqnarray*}
and that
\begin{eqnarray*}
  u (t) & = & \lambda^{2 / 3} \left( \frac{\exp (2 \beta t)}{2} \right)^{2 /
  3}
\end{eqnarray*}
\begin{eqnarray*}
  \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} \frac{\exp (2 \beta t)}{\sqrt{u
  (t)}} & = & \left[ \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} \frac{\exp (2
  \beta t)^{2 / 3}}{2^{- 1 / 3}} \right] \lambda^{- 1 / 3}\\
  & = & \theta e^{- \beta} .
\end{eqnarray*}
Hence we have $\lambda^{\ast 2 / 3} = \frac{\left( \sum_{t = 0}^{\sqrt{\log
(\alpha)} - 1} \frac{\exp (2 \beta t)^{2 / 3}}{2^{- 1 / 3}}
\right)^2}{\theta^2 e^{- 2 \beta}}$ and
\begin{eqnarray*}
  u (t) & = & \frac{\left( \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} \exp (2
  \beta t)^{2 / 3} \right)^2}{\theta^2 e^{- 2 \beta}} (\exp (2 \beta t))^{2 /
  3},
\end{eqnarray*}
giving
\begin{eqnarray*}
  \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} u (t) & = & \frac{1}{\theta^2 e^{- 2
  \beta}} \left( \sum_{t = 0}^{\sqrt{\log (\alpha)} - 1} \exp (4 \beta t / 3)
  \right)^2\\
  & = & \frac{1}{\theta^2 e^{- 2 \beta}} \left( \sum_{t = 0}^{\sqrt{\log
  (\alpha)} - 1} \exp (4 \beta / 3)^t \right)^2\\
  & = & \frac{1}{\theta^2 e^{- 2 \beta}} \left( \sum_{t = 0}^{\sqrt{\log
  (\alpha)} - 1} \exp (4 \beta / 3)^t \right)^2\\
  & = & \frac{1}{\theta^2 e^{- 2 \beta}} \left( \frac{\exp (4 \beta /
  3)^{\sqrt{\log (\alpha)}} - 1 }{\exp (4 \beta / 3) - 1} \right)^2\\
  & = & \frac{1}{\theta^2} \frac{\exp (2 \beta) \left( \exp (4 \beta /
  3)^{\sqrt{\log (\alpha)}} - 1  \right)^2}{[\exp (4 \beta / 3) - 1]^2} .
\end{eqnarray*}
For some given $\beta$, we get the total complexity of
\[ \frac{128 L^2}{3 \mu^2 \delta^2} \cdummy \frac{\exp (2 \beta) \left( \exp
   (4 \beta / 3)^{\sqrt{\log (\alpha)}} - 1  \right)^2}{[\exp (4 \beta / 3) -
   1]^2} =\mathcal{O} \left( \frac{128 L^2}{3 \mu^2 \delta^2} e^{\sqrt{\log
   (R_0 / \varepsilon)}} \right) \]


\

\

\

[1] Bertsekas, Dimitri.~\tmtextit{Convex optimization algorithms}. Athena
Scientific, 2015.

[2] Davis, D. , D. Drusvyatskiy , and V Charisopoulos. ``Stochastic algorithms
with geometric step decay converge linearly on sharp
functions."~\tmtextit{arXiv}~(2019).

[3] Davis, Damek, et al. ``Subgradient methods for sharp weakly convex
functions."~\tmtextit{Journal of Optimization Theory and Applications}~179.3
(2018): 962-982.

\

\

\

\end{document}
